{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "trusted": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Processing...\n100%|██████████████████████████████████████████| 2/2 [00:00<00:00, 67650.06it/s]\n"
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 77>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     81\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m# prepare dara\u001B[39;00m\n\u001B[0;32m---> 84\u001B[0m train_data \u001B[38;5;241m=\u001B[39m \u001B[43mGraphDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTRAIN_DIR\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshuffle()\n\u001B[1;32m     85\u001B[0m val_data \u001B[38;5;241m=\u001B[39m GraphDataset(VAL_DIR)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03mif small_dataset:\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;124;03m    train_loader = DataListLoader(train_data[:1000], batch_size=batch_size, shuffle=True)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;124;03m    val_loader = DataListLoader(val_data, batch_size=batch_size)\u001B[39;00m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/FALL/vectornet/dataset.py:52\u001B[0m, in \u001B[0;36mGraphDataset.__init__\u001B[0;34m(self, root, transform, pre_transform)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, root, transform\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, pre_transform\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 52\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mGraphDataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_transform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslices \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_paths[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:50\u001B[0m, in \u001B[0;36mInMemoryDataset.__init__\u001B[0;34m(self, root, transform, pre_transform, pre_filter)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, root: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     47\u001B[0m              transform: Optional[Callable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     48\u001B[0m              pre_transform: Optional[Callable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     49\u001B[0m              pre_filter: Optional[Callable] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 50\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_transform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_filter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/data/dataset.py:87\u001B[0m, in \u001B[0;36mDataset.__init__\u001B[0;34m(self, root, transform, pre_transform, pre_filter)\u001B[0m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_download()\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 87\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch_geometric/data/dataset.py:170\u001B[0m, in \u001B[0;36mDataset._process\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mProcessing...\u001B[39m\u001B[38;5;124m'\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mstderr)\n\u001B[1;32m    169\u001B[0m makedirs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_dir)\n\u001B[0;32m--> 170\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    172\u001B[0m path \u001B[38;5;241m=\u001B[39m osp\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessed_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre_transform.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    173\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(_repr(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_transform), path)\n",
      "File \u001B[0;32m~/FALL/vectornet/dataset.py:115\u001B[0m, in \u001B[0;36mGraphDataset.process\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# [x, y, cluster, edge_index, valid_len]\u001B[39;00m\n\u001B[1;32m    114\u001B[0m g_ls \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 115\u001B[0m padd_to_index \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalid_len_ls\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m feature_len \u001B[38;5;241m=\u001B[39m data_ls[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ind, tup \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_ls):\n",
      "File \u001B[0;32m<__array_function__ internals>:5\u001B[0m, in \u001B[0;36mamax\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2705\u001B[0m, in \u001B[0;36mamax\u001B[0;34m(a, axis, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2589\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_amax_dispatcher)\n\u001B[1;32m   2590\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mamax\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue, initial\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue,\n\u001B[1;32m   2591\u001B[0m          where\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue):\n\u001B[1;32m   2592\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2593\u001B[0m \u001B[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001B[39;00m\n\u001B[1;32m   2594\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2703\u001B[0m \u001B[38;5;124;03m    5\u001B[39;00m\n\u001B[1;32m   2704\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2705\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaximum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2706\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:87\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     84\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     85\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 87\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2020-05-27 15:00\n",
    "# @Author  : Xiaoke Huang\n",
    "# @Email   : xiaokehuang@foxmail.com\n",
    "from modeling.vectornet import HGNN\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.viz_utils import show_predict_result\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pdb\n",
    "import os\n",
    "from dataset import GraphDataset\n",
    "#from torch_geometric.data import DataLoader, DataListLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from utils.eval import get_eval_metric_results\n",
    "from tqdm import tqdm\n",
    "import torch_geometric.nn as nn\n",
    "import time\n",
    "\n",
    "# %%\n",
    "# train related\n",
    "TRAIN_DIR = os.path.join('interm_data', 'train_intermediate')\n",
    "VAL_DIR = os.path.join('interm_data', 'val_intermediate')\n",
    "gpus = [2, 3]\n",
    "SEED = 13\n",
    "epochs = 25\n",
    "batch_size = 4096 * len(gpus)\n",
    "decay_lr_factor = 0.3\n",
    "decay_lr_every = 5\n",
    "lr = 0.001\n",
    "in_channels, out_channels = 8, 60\n",
    "show_every = 20\n",
    "val_every = 5\n",
    "small_dataset = True\n",
    "end_epoch = 0\n",
    "save_dir = 'trained_params'\n",
    "best_minade = float('inf')\n",
    "global_step = 0\n",
    "date = '20221107'\n",
    "# eval related\n",
    "max_n_guesses = 1\n",
    "horizon = 30\n",
    "miss_threshold = 2.0\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_dir, model, optimizer, end_epoch, val_minade, date):\n",
    "    # state_dict: a Python dictionary object that:\n",
    "    # - for a model, maps each layer to its parameter tensor;\n",
    "    # - for an optimizer, contains info about the optimizerâ€™s states and hyperparameters used.\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    state = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'end_epoch': end_epoch,\n",
    "        'val_minade': val_minade\n",
    "    }\n",
    "    checkpoint_path = os.path.join(checkpoint_dir,\n",
    "                                   f'epoch_{end_epoch}.valminade_{val_minade:.3f}.{date}.{\"xkhuang\"}.pth')\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)\n",
    "    return checkpoint_path['end_epoch']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # training envs\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # prepare dara\n",
    "    train_data = GraphDataset(TRAIN_DIR).shuffle()\n",
    "    val_data = GraphDataset(VAL_DIR)\n",
    "    \"\"\"\n",
    "    if small_dataset:\n",
    "        train_loader = DataListLoader(train_data[:1000], batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataListLoader(val_data[:200], batch_size=batch_size)\n",
    "    else:\n",
    "        train_loader = DataListLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataListLoader(val_data, batch_size=batch_size)\n",
    "    \"\"\"\n",
    "    if small_dataset:\n",
    "        train_loader = DataLoader(train_data[:1000], batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data[:200], batch_size=batch_size)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    model = HGNN(in_channels, out_channels)\n",
    "    print(model.eval())\n",
    "    #model = nn.DataParallel(model, device_ids=gpus, output_device=gpus[0])\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "    writer = SummaryWriter('tensorboard/20221107')\n",
    "    # data_sample = next(iter(train_loader))\n",
    "    # writer.add_graph(model,data_sample) # 这2行确实跑不了，因为需要额外的插件\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=decay_lr_every, gamma=decay_lr_factor)\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        acc_loss = .0\n",
    "        num_samples = 0\n",
    "        start_tic = time.time()\n",
    "        for data in train_loader:\n",
    "            data.to(device)\n",
    "            if epoch < end_epoch: break\n",
    "            #y = torch.cat([i.y for i in data], 0).view(-1, out_channels).to(device)\n",
    "            #print(data.y)\n",
    "            y = torch.cat([data.y], 0).view(-1, out_channels).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = F.mse_loss(out, y)\n",
    "            loss.backward()\n",
    "            acc_loss += batch_size * loss.item()\n",
    "            num_samples += y.shape[0]\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            if (global_step + 1) % show_every == 0:\n",
    "                print(\n",
    "                    f\"loss at epoch {epoch} step {global_step}:{loss.item():3f}, lr:{optimizer.state_dict()['param_groups'][0]['lr']: .6f}, time:{time.time() - start_tic: 4f}sec\")\n",
    "        scheduler.step()\n",
    "        print(\n",
    "            f\"loss at epoch {epoch}:{acc_loss / num_samples:.3f}, lr:{optimizer.state_dict()['param_groups'][0]['lr']: .6f}, time:{time.time() - start_tic: 4f}sec\")\n",
    "\n",
    "        if (epoch + 1) % val_every == 0 and (not epoch < end_epoch):\n",
    "            print(\"eval as epoch:{epoch}\")\n",
    "            metrics = get_eval_metric_results(model, val_loader, device, out_channels, max_n_guesses, horizon,\n",
    "                                              miss_threshold)\n",
    "            curr_minade = metrics[\"minADE\"]\n",
    "            print(f\"minADE:{metrics['minADE']:3f}, minFDE:{metrics['minFDE']:3f}, MissRate:{metrics['MR']:3f}\")\n",
    "\n",
    "            if curr_minade < best_minade:\n",
    "                best_minade = curr_minade\n",
    "                save_checkpoint(save_dir, model, optimizer, epoch, best_minade, date)\n",
    "\n",
    "    # eval result on the identity dataset\n",
    "    metrics = get_eval_metric_results(model, val_loader, device, out_channels, max_n_guesses, horizon, miss_threshold)\n",
    "    curr_minade = metrics[\"minADE\"]\n",
    "    if curr_minade < best_minade:\n",
    "        best_minade = curr_minade\n",
    "        save_checkpoint(save_dir, model, optimizer, -1, best_minade, date)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "trusted": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/zhuhe/anaconda3/envs/pytorch/bin/python3'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "trusted": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lyft: Comprehensive guide to start competition\n",
    "\n",
    "![](http://www.l5kit.org/_images/av.jpg)\n",
    "<cite>The image from L5Kit official document: <a href=\"http://www.l5kit.org/README.html\">http://www.l5kit.org/README.html</a></cite>\n",
    "\n",
    "In this kernel, I will explain how to setup develop environment & the data for this competition.<br/>\n",
    "\n",
    "The dataset structure is a bit complicated since the Lyft Level 5 dataset contains various kinds of information.\n",
    "Lyft provided a useful library to deal with this data, we need to learn how to fully-utilize these provided tools at first!"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Competition description\n",
    "\n",
    " - Official page: [https://self-driving.lyft.com/level5/prediction/](https://self-driving.lyft.com/level5/prediction/)\n",
    "\n",
    "<blockquote>\n",
    "    The dataset consists of 170,000 scenes capturing the environment around the autonomous vehicle. Each scene encodes the state of the vehicleâ€™s surroundings at a given point in time.\n",
    "</blockquote>\n",
    "\n",
    "<div style=\"clear:both;display:table\">\n",
    "<img src=\"https://self-driving.lyft.com/wp-content/uploads/2020/06/motion_dataset_lrg_redux.gif\" style=\"width:45%;float:left\"/>\n",
    "<img src=\"https://self-driving.lyft.com/wp-content/uploads/2020/06/motion_dataset_2-1.png\" style=\"width:45%;float:left\"/>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "<p><b>The goal of this competition is to predict other car/cyclist/pedestrian (called \"agent\")'s motion.</b><p>\n",
    "\n",
    "<img src=\"https://self-driving.lyft.com/wp-content/uploads/2020/06/diagram-prediction-1.jpg\" style=\"width:70%\"/>\n",
    "\n",
    "    \n",
    "<p><a href=\"https://vimeo.com/389096888\">Lyft Self-Driving Employee Rides Testing</a> from <a href=\"https://vimeo.com/user99616812\">Lyft Level 5</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://player.vimeo.com/video/389096888', width=640, height=360, frameborder=\"0\", allow=\"autoplay; fullscreen\", allowfullscreen=True)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Environment setup\n",
    "\n",
    "Following instruction will work only on kaggle kernel.<br/>\n",
    "Please try following this step by creating new kernel (or forking this kernel) on kaggle.\n",
    "\n",
    "## Install the additional library: l5kit\n",
    "We want to install `l5kit` library, which is provided by Lyft to handle this competition's dataset (Level5 Prediction Dataset).<br/>\n",
    "Here I use the \"Utility script\" functionality of kaggle kernel, instead of installing the library by `pip install` command.\n",
    "\n",
    "\n",
    "Click \"File\" botton on top-left, and choose \"Add utility script\". For the pop-up search window, you need to remove \"Your Work\" filter, and search \"[philculliton/kaggle-l5kit](https://www.kaggle.com/mathurinache/kaggle-l5kit)\" on top-right of the search window.\n",
    "Then you can add the **kaggle-l5kit** utility script.\n",
    "\n",
    "If successful, you can see \"usr/lib/kaggle-l5kit\" is added to the \"Data\" section of this kernel page on right side of the kernel.\n",
    "\n",
    "Reference for utility script\n",
    " - [Feature Launch: Import scripts into notebook kernels](https://www.kaggle.com/product-feedback/91185)\n",
    " - [Import functions from Kaggle script](https://www.kaggle.com/rtatman/import-functions-from-kaggle-script)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Running this pip install code takes time, we can skip it when we attach utility script correctly!\n",
    "# !pip install -U l5kit"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import l5kit\n",
    "\n",
    "l5kit.__version__"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.5.0'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, we don't need to hassle with the time to run `pip install` which takes time to install the library. Just attach the utility script and the library is already setup!"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attach additional dataset source: for config files\n",
    "\n",
    "We are using some config files when we want to load/visualize Lyft level5 dataset.<br/>\n",
    "@jpbremer already uploaded config files as Kaggle Dataset platform: [lyft-config-files](https://www.kaggle.com/jpbremer/lyft-config-files).<br/>\n",
    "This is originally from official github [lyft/l5kit example page](https://github.com/lyft/l5kit/tree/master/examples).\n",
    "\n",
    "Click \"Add data\" button and press \"Search by URL\". Typing \"https://www.kaggle.com/jpbremer/lyft-config-files\" shows the dataset.<br/>\n",
    "Once the dataset is successfully added you can see \"lyft-config-files\" as dataset on left side bar."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## import"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "pd.set_option('max_columns', 50)"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16568\\109800024.py:16: DeprecationWarning:\n",
      "\n",
      "Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": "        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "rc('animation', html='jshtml')"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01ml5kit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChunkedDataset, LocalDataManager\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01ml5kit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EgoDataset, AgentDataset\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01ml5kit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrasterization\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m build_rasterizer\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\l5kit\\data\\__init__.py:7\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlabels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PERCEPTION_LABEL_TO_INDEX, PERCEPTION_LABELS, TL_FACE_LABEL_TO_INDEX, TL_FACE_LABELS\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlocal_data_manager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataManager, LocalDataManager\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmap_api\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MapAPI\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mzarr_dataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AGENT_DTYPE, ChunkedDataset, FRAME_DTYPE, SCENE_DTYPE, TL_FACE_DTYPE\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mzarr_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m zarr_concat\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\l5kit\\data\\map_api.py:12\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01ml5kit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataManager\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeometry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transform_points\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mroad_network_pb2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GeoFrame, GlobalId, MapElement, MapFragment\n\u001B[0;32m     15\u001B[0m CACHE_SIZE \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;241m1e5\u001B[39m)\n\u001B[0;32m     16\u001B[0m ENCODING \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\l5kit\\data\\proto\\road_network_pb2.py:36\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m _sym_db \u001B[38;5;241m=\u001B[39m _symbol_database\u001B[38;5;241m.\u001B[39mDefault()\n\u001B[0;32m     17\u001B[0m DESCRIPTOR \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mFileDescriptor(\n\u001B[0;32m     18\u001B[0m   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mroad_network.proto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     19\u001B[0m   package\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml5kit.maps\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m   ,\n\u001B[0;32m     25\u001B[0m   dependencies\u001B[38;5;241m=\u001B[39m[google_dot_protobuf_dot_descriptor__pb2\u001B[38;5;241m.\u001B[39mDESCRIPTOR,google_dot_protobuf_dot_empty__pb2\u001B[38;5;241m.\u001B[39mDESCRIPTOR,])\n\u001B[0;32m     29\u001B[0m _ACCESSRESTRICTION_TYPE \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mEnumDescriptor(\n\u001B[0;32m     30\u001B[0m   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mType\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     31\u001B[0m   full_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml5kit.maps.AccessRestriction.Type\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     32\u001B[0m   filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     33\u001B[0m   file\u001B[38;5;241m=\u001B[39mDESCRIPTOR,\n\u001B[0;32m     34\u001B[0m   create_key\u001B[38;5;241m=\u001B[39m_descriptor\u001B[38;5;241m.\u001B[39m_internal_create_key,\n\u001B[0;32m     35\u001B[0m   values\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m---> 36\u001B[0m     \u001B[43m_descriptor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEnumValueDescriptor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mUNKNOWN\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m      \u001B[49m\u001B[43mserialized_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcreate_key\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_descriptor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_internal_create_key\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     41\u001B[0m     _descriptor\u001B[38;5;241m.\u001B[39mEnumValueDescriptor(\n\u001B[0;32m     42\u001B[0m       name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNO_RESTRICTION\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     43\u001B[0m       serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     44\u001B[0m       \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     45\u001B[0m       create_key\u001B[38;5;241m=\u001B[39m_descriptor\u001B[38;5;241m.\u001B[39m_internal_create_key),\n\u001B[0;32m     46\u001B[0m     _descriptor\u001B[38;5;241m.\u001B[39mEnumValueDescriptor(\n\u001B[0;32m     47\u001B[0m       name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mONLY_HOV\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m     48\u001B[0m       serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     49\u001B[0m       \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     50\u001B[0m       create_key\u001B[38;5;241m=\u001B[39m_descriptor\u001B[38;5;241m.\u001B[39m_internal_create_key),\n\u001B[0;32m     51\u001B[0m     _descriptor\u001B[38;5;241m.\u001B[39mEnumValueDescriptor(\n\u001B[0;32m     52\u001B[0m       name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mONLY_BUS\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[0;32m     53\u001B[0m       serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     54\u001B[0m       \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     55\u001B[0m       create_key\u001B[38;5;241m=\u001B[39m_descriptor\u001B[38;5;241m.\u001B[39m_internal_create_key),\n\u001B[0;32m     56\u001B[0m     _descriptor\u001B[38;5;241m.\u001B[39mEnumValueDescriptor(\n\u001B[0;32m     57\u001B[0m       name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mONLY_BIKE\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[0;32m     58\u001B[0m       serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     59\u001B[0m       \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     60\u001B[0m       create_key\u001B[38;5;241m=\u001B[39m_descriptor\u001B[38;5;241m.\u001B[39m_internal_create_key),\n\u001B[0;32m     61\u001B[0m     _descriptor\u001B[38;5;241m.\u001B[39mEnumValueDescriptor(\n\u001B[0;32m     62\u001B[0m       name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mONLY_TURN\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,\n\u001B[0;32m     63\u001B[0m       serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     64\u001B[0m       \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     65\u001B[0m       create_key\u001B[38;5;241m=\u001B[39m_descriptor\u001B[38;5;241m.\u001B[39m_internal_create_key),\n\u001B[0;32m     66\u001B[0m   ],\n\u001B[0;32m     67\u001B[0m   containing_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     68\u001B[0m   serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     69\u001B[0m   serialized_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m575\u001B[39m,\n\u001B[0;32m     70\u001B[0m   serialized_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m672\u001B[39m,\n\u001B[0;32m     71\u001B[0m )\n\u001B[0;32m     72\u001B[0m _sym_db\u001B[38;5;241m.\u001B[39mRegisterEnumDescriptor(_ACCESSRESTRICTION_TYPE)\n\u001B[0;32m     74\u001B[0m _DAILYTIMEINTERVAL_DAYOFTHEWEEK \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mEnumDescriptor(\n\u001B[0;32m     75\u001B[0m   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDayOfTheWeek\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     76\u001B[0m   full_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml5kit.maps.DailyTimeInterval.DayOfTheWeek\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    120\u001B[0m   serialized_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m976\u001B[39m,\n\u001B[0;32m    121\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\protobuf\\descriptor.py:755\u001B[0m, in \u001B[0;36mEnumValueDescriptor.__new__\u001B[1;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001B[0m\n\u001B[0;32m    752\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, name, index, number,\n\u001B[0;32m    753\u001B[0m             \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,  \u001B[38;5;66;03m# pylint: disable=redefined-builtin\u001B[39;00m\n\u001B[0;32m    754\u001B[0m             options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, create_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 755\u001B[0m   \u001B[43m_message\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMessage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_CheckCalledFromGeneratedFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    756\u001B[0m   \u001B[38;5;66;03m# There is no way we can build a complete EnumValueDescriptor with the\u001B[39;00m\n\u001B[0;32m    757\u001B[0m   \u001B[38;5;66;03m# given parameters (the name of the Enum is not known, for example).\u001B[39;00m\n\u001B[0;32m    758\u001B[0m   \u001B[38;5;66;03m# Fortunately generated files just pass it to the EnumDescriptor()\u001B[39;00m\n\u001B[0;32m    759\u001B[0m   \u001B[38;5;66;03m# constructor, which will ignore it, so returning None is good enough.\u001B[39;00m\n\u001B[0;32m    760\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    filenames.sort()\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic tutorial\n",
    "\n",
    "Before going to look the data, we still need to learn some advanced functionality to handle data.\n",
    "\n",
    "This section basically follows [Data format](https://github.com/lyft/l5kit/blob/master/data_format.md) description page of l5kit.\n",
    "\n",
    "## numpy structured array\n",
    "\n",
    "Lyft dataset uses numpy's [structured array](https://docs.scipy.org/doc/numpy/user/basics.rec.html) functionality to store various kinds of features together.<br/>\n",
    "Let's see example to how it works"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "my_arr = np.zeros(3, dtype=[(\"color\", (np.uint8, 3)), (\"label\", np.bool)])\n",
    "\n",
    "my_arr[0][\"color\"] = [0, 218, 130]\n",
    "my_arr[0][\"label\"] = True\n",
    "my_arr[1][\"color\"] = [245, 59, 255]\n",
    "my_arr[1][\"label\"] = True\n",
    "\n",
    "my_arr"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "At top line we defined length=3 array. Usually each element of array consists of only integer or float, but we can define custom structured format by specifying `dtype` as list of \"fields\" which consists of name and structure.\n",
    "\n",
    "Above example contains 2 fields. 1. 8byte uint with length 3 array, 2. single element boolean array.\n",
    "\n",
    "As you can see, `my_arr[i][\"name\"]` will access i-th element's \"name\" field.\n",
    "\n",
    "Usually when we train neural network, we would like to access all the field of random i-th element.\n",
    "According to [Lyft Data Format page](https://github.com/lyft/l5kit/blob/master/data_format.md), \"Structured arrays are a great fit to group this data together in memory and on disk.\", rather than preparing \"color\" array and \"label\" array separately and access each array's i-th element, especially when the number of field glow. "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## zarr"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zarr data format is used to store and read these numpy structured arrays from disk.<br/>\n",
    "Zarr allows us to write very large (structured) arrays to disk in n-dimensional compressed chunks.\n",
    "\n",
    "Here is a short tutorial:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import zarr\n",
    "\n",
    "z = zarr.open(\"./dataset.zarr\", mode=\"w\", shape=(500,), dtype=np.float32, chunks=(100,))\n",
    "\n",
    "# We can write to it by assigning to it. This gets persisted on disk.\n",
    "z[0:150] = np.arange(150)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we specified chunks to be of size 100, we just wrote to two separate chunks. On your filesystem in the dataset.zarr folder you will now find these two chunks. As we didn't completely fill the second chunk, those missing values will be set to the fill value (defaults to 0). The chunks are actually compressed on disk too! \n",
    "\n",
    "We can print some info: by not doing much work at all we saved almost 75% in disk space!"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(z.info)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we check filesystem, `dataset.zarr` directory is created and there are 2 files \"0\" and \"1\" which are chunks currently created by just assigning value to zarr array."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!ls -l ./*"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading from a zarr array is as easy as slicing from it like you would any numpy array. The return value is an ordinary numpy array. Zarr takes care of determining which chunks to read from."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(z[::20]) # Read every 20th value"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zarr supports StructuredArrays, the data format we use for our datasets are a set of structured arrays stored in zarr format.\n",
    "\n",
    "Some other zarr benefits are:\n",
    "\n",
    " - Safe to use in a multithreading or multiprocessing setup. Reading is entirely safe, for writing there are lock mechanisms built-in.\n",
    " - If you have a dataset that is too large to fit in memory, loading a single sample becomes my_sample = z[sample_index] and you get compression out of the box.\n",
    " - The blosc compressor is so fast that it is faster to read the compressed data and uncompress it than reading the uncompressed data from disk.\n",
    " - Zarr supports multiple backend stores, your data could also live in a zip file, or even a remote server or S3 bucket.\n",
    " - Other libraries such as xarray, Dask and TensorStore have good interoperability with Zarr.\n",
    " - The metadata (e.g. dtype, chunk size, compression type) is stored inside the zarr dataset too. If one day you decide to change your chunk size, you can still read the older datasets without changing any code."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See the zarr [docs](https://zarr.readthedocs.io/en/stable/) for more details."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lyft's dataset structure\n",
    "\n",
    "Now the basics learning are done! We can start looking Lyft level 5 dataset using `l5kit` library.\n",
    "\n",
    "I referenced [l5kit visualize_data.ipynb](https://github.com/lyft/l5kit/blob/master/examples/visualisation/visualise_data.ipynb) for this section.\n",
    "\n",
    "\n",
    "## Word definition\n",
    " - **\"Ego\"** is the host car which is recording/measuring the dataset.\n",
    " - **\"Agent\"** is the surronding car except \"Ego\" car.\n",
    " - **\"Frame\"** is the 1 image snapshot, where **\"Scene\"** is made of multiple frames of contious-time (video).\n",
    "\n",
    "## Class diagram\n",
    "<img src=\"https://storage.googleapis.com/kaggle-forum-message-attachments/987047/16744/l5kit_class.png\" width=\"600\" />\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Initial setup"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\"\n",
    "# get config\n",
    "cfg = load_config_data(\"/kaggle/input/lyft-config-files/visualisation_config.yaml\")\n",
    "print(cfg)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading data\n",
    "\n",
    "Here we will only use the first dataset from the sample set.\n",
    "\n",
    "We're building a `LocalDataManager` object. This will resolve relative paths from the config using the `L5KIT_DATA_FOLDER` env variable we have just set.\n",
    "\n",
    "Here sample.zarr data is used for visualization, please use train.zarr / validate.zarr / test.zarr for actual model training/validation/prediction.\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dm = LocalDataManager()\n",
    "dataset_path = dm.require('scenes/sample.zarr')\n",
    "zarr_dataset = ChunkedDataset(dataset_path)\n",
    "zarr_dataset.open()\n",
    "print(zarr_dataset)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_path"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with the raw data\n",
    "\n",
    "`zarr_dataset` contains **scenes, frames, agents, tl_faces** attributes, which are the raw structured array data.\n",
    "\n",
    "Each data structure definition can be checked at [here](https://github.com/lyft/l5kit/blob/master/data_format.md#2020-lyft-competition-dataset-format).\n",
    "\n",
    "### scenes\n",
    "\n",
    "```\n",
    "SCENE_DTYPE = [\n",
    "    (\"frame_index_interval\", np.int64, (2,)),\n",
    "    (\"host\", \"<U16\"),  # Unicode string up to 16 chars\n",
    "    (\"start_time\", np.int64),\n",
    "    (\"end_time\", np.int64),\n",
    "]\n",
    "```\n",
    "\n",
    "### frames\n",
    "\n",
    "```\n",
    "FRAME_DTYPE = [\n",
    "    (\"timestamp\", np.int64),\n",
    "    (\"agent_index_interval\", np.int64, (2,)),\n",
    "    (\"traffic_light_faces_index_interval\", np.int64, (2,)),\n",
    "    (\"ego_translation\", np.float64, (3,)),\n",
    "    (\"ego_rotation\", np.float64, (3, 3)),\n",
    "]\n",
    "```\n",
    "\n",
    "### agents\n",
    "\n",
    "```\n",
    "AGENT_DTYPE = [\n",
    "    (\"centroid\", np.float64, (2,)),\n",
    "    (\"extent\", np.float32, (3,)),\n",
    "    (\"yaw\", np.float32),\n",
    "    (\"velocity\", np.float32, (2,)),\n",
    "    (\"track_id\", np.uint64),\n",
    "    (\"label_probabilities\", np.float32, (len(LABELS),)),\n",
    "]\n",
    "```\n",
    "\n",
    "### traffic_light_faces\n",
    "\n",
    "```\n",
    "TL_FACE_DTYPE = [\n",
    "    (\"face_id\", \"<U16\"),\n",
    "    (\"traffic_light_id\", \"<U16\"),\n",
    "    (\"traffic_light_face_status\", np.float32, (len(TL_FACE_LABELS,))),\n",
    "]\n",
    "```"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an example, we will try scatter plot using **frames \"ego_translation\"** data. This is the movement of ego car."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "frames = zarr_dataset.frames\n",
    "\n",
    "## This is slow.\n",
    "# coords = np.zeros((len(frames), 2))\n",
    "# for idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n",
    "#     frame = zarr_dataset.frames[idx_data]\n",
    "#     coords[idx_coord] = frame[\"ego_translation\"][:2]\n",
    "\n",
    "# This is much faster!\n",
    "coords = frames[\"ego_translation\"][:, :2]\n",
    "\n",
    "plt.scatter(coords[:, 0], coords[:, 1], marker='.')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-2500, 1600])\n",
    "axes.set_ylim([-2500, 1600])\n",
    "plt.title(\"ego_translation of frames\")"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Note] Performance-aware slicing\n",
    "\n",
    "I commented out some codes which uses for loop. This is slow because when we call `zarr_dataset.frames[idx_data]`, **data is decompressed everytime** using zarr format.<br/>\n",
    "Instead, we can remove for loop by using slice accessing. `frames[\"ego_translation\"]` is same as `frames[:][\"ego_translation\"]`, which accesses all the element's \"ego_translation\" field. By writing this, number of decompression call is reduced and code runs faster dramatically."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pytorch Dataset class\n",
    "\n",
    "Instead of working with raw data, L5Kit provides PyTorch ready datasets.\n",
    "It's much easier to use this wrapped dataset class to access data.\n",
    "\n",
    "2 dataset class is implemented.\n",
    "\n",
    " - **EgoDataset**: this dataset iterates over the AV (Autonomous Vehicle) annotations\n",
    " - **AgentDataset**: this dataset iterates over other agents annotations"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 'map_type': 'py_semantic' for cfg.\n",
    "semantic_rasterizer = build_rasterizer(cfg, dm)\n",
    "semantic_dataset = EgoDataset(cfg, zarr_dataset, semantic_rasterizer)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rasterizer is in charge of visualizing the data, as we will see next."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization example\n",
    "\n",
    "Lyft l5kit also provides visualization functionalities.<br/>\n",
    "We will visualize data to understand what kind of information is stored in this dataset.<br/>"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can sample a data from dataset, and convert to RGB image using `rasterizer`.\n",
    "\n",
    " - image: (channel, height, width) image of a frame. This is Birds-eye-view (BEV) representation.\n",
    " - target_positions: (n_frames, 2) displacements in meters in world coordinates\n",
    " - target_yaws: (n_frames, 1)\n",
    " - centroid: (2) center position x&y.\n",
    " - world_to_image: (3, 3) 3x3 matrix, used for transform matrix.\n",
    "\n",
    "Data is represented as 2.5D, positions and yaws are separated.<br/>\n",
    "target_positions are represented in world coordinates and it is converted to pixel coordinates to visualize below:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_trajectory(dataset, index, title=\"target_positions movement with draw_trajectory\"):\n",
    "    data = dataset[index]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.imshow(im[::-1])\n",
    "    plt.show()\n",
    "\n",
    "visualize_trajectory(semantic_dataset, index=0)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can switch rasterizer to visualize satellite image easily!"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# map_type was changed from 'py_semantic' to 'py_satellite'.\n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\n",
    "satellite_rasterizer = build_rasterizer(cfg, dm)\n",
    "satellite_dataset = EgoDataset(cfg, zarr_dataset, satellite_rasterizer)\n",
    "\n",
    "visualize_trajectory(satellite_dataset, index=0)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "type(satellite_rasterizer), type(semantic_rasterizer)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we visualized **EgoDataset**.\n",
    "\n",
    "**AgentDataset** can be used to visualize an agent. This dataset iterates over agents and not the AV anymore, and the first one happens to be the pace car (you will see this one around a lot in the dataset)."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "agent_dataset = AgentDataset(cfg, zarr_dataset, satellite_rasterizer)\n",
    "visualize_trajectory(agent_dataset, index=0)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can compare above 2 images, only the target car changed from host to agent."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<blockquote> <b>System Origin and Orientation</b>\n",
    "\n",
    "At this point you may have noticed that we flip the image on the Y-axis before plotting it.\n",
    "\n",
    "When moving from 3D to 2D we stick to a right-hand system, where the origin is in the bottom-left corner with positive x-values going right and positive y-values going up the image plane. The camera is facing down the negative z axis.\n",
    "\n",
    "However, both opencv and pyplot place the origin in the top-left corner with positive x going right and positive y going down in the image plane. The camera is facing down the positive z-axis.\n",
    "\n",
    "The flip done on the resulting image is for visualisation purposes to accommodate the difference in the two coordinate frames.\n",
    "\n",
    "Further, all our rotations are counter-clockwise for positive value of the angle.\n",
    "</blockquote>\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scene handling\n",
    "\n",
    "Both EgoDataset and AgentDataset provide 2 methods for getting interesting indices:\n",
    "\n",
    " - **get_frame_indices** returns the indices for a given frame. For the `EgoDataset` this matches a single observation, while more than one index could be available for the `AgentDataset`, as that given frame may contain more than one valid agent\n",
    " - **get_scene_indices** returns indices for a given scene. For both datasets, these might return more than one index"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "dataset = semantic_dataset\n",
    "scene_idx = 34\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "    clear_output(wait=True)\n",
    "    images.append(PIL.Image.fromarray(im[::-1]))"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# From https://www.kaggle.com/jpbremer/lyft-scene-visualisations by @jpbremer\n",
    "def animate_solution(images):\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(images[i])\n",
    "        return (im,)\n",
    " \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(images[0])\n",
    "    def init():\n",
    "        im.set_data(images[0])\n",
    "        return (im,)\n",
    "    \n",
    "    return animation.FuncAnimation(fig, animate, init_func=init, frames=len(images), interval=60, blit=True)\n",
    "\n",
    "anim = animate_solution(images)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "HTML(anim.to_jshtml())"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next to go\n",
    "\n",
    "That's all for first introduction to familialize with l5kit dataset.<br/>\n",
    "We have still need to know a lot to write data processing code to build a better prediction model pipeline.<br/>\n",
    "I wrote next kernel [Lyft: Deep into the l5kit library](https://www.kaggle.com/corochann/lyft-deep-into-the-l5kit-library) for this purpose.\n",
    "\n",
    "If you don't want quickly try training prediction model baseline, you can go to [agent motion prediction ipynb](https://github.com/lyft/l5kit/tree/master/examples/agent_motion_prediction) to start prediction.<br/>\n",
    "The same content is already uploaded: [a port of l5kit example](https://www.kaggle.com/hirune924/just-a-port-of-l5kit-example/data) by @hirune924.\n",
    "\n",
    "**[Update] I also wrote training tutorial kernel:**\n",
    " - [Lyft: Training with multi-mode confidence](https://www.kaggle.com/corochann/lyft-training-with-multi-mode-confidence)\n",
    " - [Lyft: Prediction with multi-mode confidence](https://www.kaggle.com/corochann/lyft-prediction-with-multi-mode-confidence)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Further reference\n",
    "\n",
    " - Paper of this Lyft Level 5 prediction dataset: [One Thousand and One Hours: Self-driving Motion Prediction Dataset](https://arxiv.org/abs/2006.14480)\n",
    " - [jpbremer/lyft-scene-visualisations](https://www.kaggle.com/jpbremer/lyft-scene-visualisations)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3 style=\"color:red\">If this kernel helps you, please upvote to keep me motivated :)<br>Thanks!</h3>"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}
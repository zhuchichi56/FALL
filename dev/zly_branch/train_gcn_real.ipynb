{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyft: Training with multi-mode confidence\n",
    "\n",
    "![](http://www.l5kit.org/_images/av.jpg)\n",
    "<cite>The image from L5Kit official document: <a href=\"http://www.l5kit.org/README.html\">http://www.l5kit.org/README.html</a></cite>\n",
    "\n",
    "Continued from the previous kernel:\n",
    " - [Lyft: Comprehensive guide to start competition](https://www.kaggle.com/corochann/lyft-comprehensive-guide-to-start-competition)\n",
    " - [Lyft: Deep into the l5kit library](https://www.kaggle.com/corochann/lyft-deep-into-the-l5kit-library)\n",
    "\n",
    "In this kernel, I will run **pytorch CNN model training**. Especially, followings are new items to try:\n",
    " - Predict **multi-mode with confidence**: As written in [evaluation metric](https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/overview/evaluation) page, we can predict **3 modes** of motion trajectory.\n",
    " - Training loss with **competition evaluation metric**\n",
    " - Use Training abstraction library **`pytorch-ignite` and `pytorch-pfn-extras`**.\n",
    "\n",
    "\n",
    "[Update 2020/9/6]<br/>\n",
    "Published prediction kernel: [Lyft: Prediction with multi-mode confidence](https://www.kaggle.com/corochann/lyft-prediction-with-multi-mode-confidence)<br/>\n",
    "Try yourself how good score you can get using only single model without ensemble! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "\n",
    " - Please add [pestipeti/lyft-l5kit-unofficial-fix](https://www.kaggle.com/pestipeti/lyft-l5kit-unofficial-fix) as utility script.\n",
    "    - Official utility script \"[philculliton/kaggle-l5kit](https://www.kaggle.com/mathurinache/kaggle-l5kit)\" does not work with pytorch GPU.\n",
    " - Please add [lyft-config-files](https://www.kaggle.com/jpbremer/lyft-config-files) as dataset\n",
    " \n",
    "See previous kernel [Lyft: Comprehensive guide to start competition](https://www.kaggle.com/corochann/lyft-comprehensive-guide-to-start-competition) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:44.949052Z",
     "iopub.status.busy": "2022-10-13T03:41:44.948344Z",
     "iopub.status.idle": "2022-10-13T03:41:46.372877Z",
     "shell.execute_reply": "2022-10-13T03:41:46.372392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_877249/1331493952.py:17: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "# pd.set_option('max_columns', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:46.376031Z",
     "iopub.status.busy": "2022-10-13T03:41:46.375772Z",
     "iopub.status.idle": "2022-10-13T03:41:47.004815Z",
     "shell.execute_reply": "2022-10-13T03:41:47.004286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l5kit version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "import l5kit\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "print(\"l5kit version:\", l5kit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:47.007432Z",
     "iopub.status.busy": "2022-10-13T03:41:47.007084Z",
     "iopub.status.idle": "2022-10-13T03:41:47.052361Z",
     "shell.execute_reply": "2022-10-13T03:41:47.051835Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "import pytorch_pfn_extras.training.extensions as E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "To define loss function to calculate competition evaluation metric **in batch**.<br/>\n",
    "It works with **pytorch tensor, so it is differentiable** and can be used for training Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:47.055363Z",
     "iopub.status.busy": "2022-10-13T03:41:47.055168Z",
     "iopub.status.idle": "2022-10-13T03:41:47.063534Z",
     "shell.execute_reply": "2022-10-13T03:41:47.062990Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Function utils ---\n",
    "# Original code from https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/metrics.py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute a negative log-likelihood for the multi-modal scenario.\n",
    "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    https://leimao.github.io/blog/LogSumExp/\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
    "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "    # print(f\"pred.shape : {pred.shape}\")\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"wrong shape for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_single(\n",
    "    gt: Tensor, pred: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
    "    # create confidence (bs)x(mode=1)\n",
    "    batch_size, future_len, num_coords = pred.shape\n",
    "    confidences = pred.new_ones((batch_size, 1))\n",
    "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "pytorch model definition. Here model outputs both **multi-mode trajectory prediction & confidence of each trajectory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:47.066130Z",
     "iopub.status.busy": "2022-10-13T03:41:47.065962Z",
     "iopub.status.idle": "2022-10-13T03:41:47.274373Z",
     "shell.execute_reply": "2022-10-13T03:41:47.273853Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Model utils ---\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "from typing import Dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:47.276881Z",
     "iopub.status.busy": "2022-10-13T03:41:47.276710Z",
     "iopub.status.idle": "2022-10-13T03:41:47.280611Z",
     "shell.execute_reply": "2022-10-13T03:41:47.280124Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Utils ---\n",
    "import yaml\n",
    "\n",
    "\n",
    "def save_yaml(filepath, content, width=120):\n",
    "    with open(filepath, 'w') as f:\n",
    "        yaml.dump(content, f, width=width)\n",
    "\n",
    "\n",
    "def load_yaml(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    return content\n",
    "\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "\n",
    "    Refer: https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary/23689767#23689767\n",
    "    \"\"\"  # NOQA\n",
    "\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:47.283628Z",
     "iopub.status.busy": "2022-10-13T03:41:47.283404Z",
     "iopub.status.idle": "2022-10-13T03:41:47.289299Z",
     "shell.execute_reply": "2022-10-13T03:41:47.288773Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Lyft configs ---\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet50',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'render_ego_history':True,\n",
    "        'step_time':0.1\n",
    "    },\n",
    "\n",
    "    'raster_params': {\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5,\n",
    "\n",
    "        'set_origin_to_bottom': True,\n",
    "        'disable_traffic_light_faces':False\n",
    "    },\n",
    "\n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 12,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'valid_data_loader': {\n",
    "        'key': 'scenes/validate.zarr',\n",
    "        'batch_size': 32,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'train_params': {\n",
    "        'max_num_steps': 10000,\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "\n",
    "        # 'eval_every_n_steps': -1\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:47.292019Z",
     "iopub.status.busy": "2022-10-13T03:41:47.291802Z",
     "iopub.status.idle": "2022-10-13T03:41:47.294975Z",
     "shell.execute_reply": "2022-10-13T03:41:47.294499Z"
    }
   },
   "outputs": [],
   "source": [
    "# 相对路径向上索引几成就是几个..\n",
    "flags_dict = {\n",
    "    \"debug\": True,\n",
    "    # --- Data configs ---\n",
    "    \"l5kit_data_folder\": \"../../../kaggle/input/lyft-motion-prediction-autonomous-vehicles\",\n",
    "    # --- Model configs ---\n",
    "    \"pred_mode\": \"multi\",\n",
    "    # --- Training configs ---\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"out_dir\": \"results/multi_train\",\n",
    "    \"epoch\": 2,\n",
    "    \"snapshot_freq\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Main script\n",
    "\n",
    "Now finished defining all the util codes. Let's start writing main script to train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading data\n",
    "\n",
    "Here we will only use the first dataset from the sample set. (sample.zarr data is used for visualization, please use train.zarr / validate.zarr / test.zarr for actual model training/validation/prediction.)<br/>\n",
    "We're building a `LocalDataManager` object. This will resolve relative paths from the config using the `L5KIT_DATA_FOLDER` env variable we have just set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:47.297906Z",
     "iopub.status.busy": "2022-10-13T03:41:47.297694Z",
     "iopub.status.idle": "2022-10-13T03:41:47.305318Z",
     "shell.execute_reply": "2022-10-13T03:41:47.304785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flags: {'debug': True, 'l5kit_data_folder': '../../../kaggle/input/lyft-motion-prediction-autonomous-vehicles', 'pred_mode': 'multi', 'device': 'cuda:0', 'out_dir': 'results/multi_train', 'epoch': 2, 'snapshot_freq': 50}\n"
     ]
    }
   ],
   "source": [
    "flags = DotDict(flags_dict)\n",
    "out_dir = Path(flags.out_dir)\n",
    "os.makedirs(str(out_dir), exist_ok=True)\n",
    "print(f\"flags: {flags_dict}\")\n",
    "save_yaml(out_dir / 'flags.yaml', flags_dict)\n",
    "save_yaml(out_dir / 'cfg.yaml', cfg)\n",
    "debug = flags.debug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:47.307611Z",
     "iopub.status.busy": "2022-10-13T03:41:47.307371Z",
     "iopub.status.idle": "2022-10-13T03:41:59.723162Z",
     "shell.execute_reply": "2022-10-13T03:41:59.722533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset...\n",
      "train_zarr <class 'l5kit.data.zarr_dataset.ChunkedDataset'>\n",
      "test_zarr <class 'l5kit.data.zarr_dataset.ChunkedDataset'>\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = flags.l5kit_data_folder\n",
    "dm = LocalDataManager(None)\n",
    "\n",
    "print(\"Load dataset...\")\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "valid_cfg = cfg[\"valid_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_path = \"scenes/train.zarr\" if debug else train_cfg[\"key\"]\n",
    "test_path = \"scenes/test.zarr\"\n",
    "train_zarr = ChunkedDataset(dm.require(train_path)).open()\n",
    "test_zarr = ChunkedDataset(dm.require(test_path)).open()\n",
    "print(\"train_zarr\", type(train_zarr))\n",
    "print(\"test_zarr\", type(test_zarr))\n",
    "train_ego_dataset = EgoDataset(cfg, train_zarr, rasterizer)\n",
    "train_agent_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "test_ego_dataset = EgoDataset(cfg, test_zarr, rasterizer)\n",
    "test_agent_dataset = AgentDataset(cfg, test_zarr, rasterizer)\n",
    "\n",
    "# valid_path = \"scenes/sample.zarr\" if debug else valid_cfg[\"key\"]\n",
    "# valid_zarr = ChunkedDataset(dm.require(valid_path)).open()\n",
    "# print(\"valid_zarr\", type(train_zarr))\n",
    "# valid_agent_dataset = AgentDataset(cfg, valid_zarr, rasterizer)\n",
    "# valid_dataset = TransformDataset(valid_agent_dataset, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:59.726574Z",
     "iopub.status.busy": "2022-10-13T03:41:59.726360Z",
     "iopub.status.idle": "2022-10-13T03:41:59.737459Z",
     "shell.execute_reply": "2022-10-13T03:41:59.736934Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_distance(centroid1, centroid2):\n",
    "    return np.sqrt(np.square(centroid1[0]-centroid2[0]) + np.square(centroid1[1]-centroid2[1]))\n",
    "# def change_all_list_to_ndarray(dic_in):\n",
    "\n",
    "def transform_one_scene_dataset(scene_ego_dataset, scene_agent_dataset, debug: False):\n",
    "    def add_element_into(ele_dict, ele):\n",
    "        temp_x = []\n",
    "        temp_x.extend(ele[\"curr_speed\"].flatten())\n",
    "        temp_x.extend(ele[\"history_positions\"].flatten())\n",
    "        temp_x.extend(ele[\"history_yaws\"].flatten())\n",
    "        temp_x.extend(ele[\"history_availabilities\"].flatten())\n",
    "        ele_dict[\"x\"].append(temp_x)\n",
    "        if len(ele_dict[\"index\"])== 0:\n",
    "            cur_index = 0\n",
    "        else:\n",
    "            cur_index = ele_dict[\"index\"][-1] + 1\n",
    "        for i in range(0,cur_index):\n",
    "            ele_dict[\"edge_attr\"].append(get_distance(ele[\"centroid\"], ele_dict[\"centroid\"][i])) # 双向图\n",
    "            ele_dict[\"edge_index\"][0].append(i)\n",
    "            ele_dict[\"edge_index\"][1].append(cur_index)\n",
    "            ele_dict[\"edge_attr\"].append(get_distance(ele[\"centroid\"], ele_dict[\"centroid\"][i]))\n",
    "            ele_dict[\"edge_index\"][1].append(i)\n",
    "            ele_dict[\"edge_index\"][0].append(cur_index)\n",
    "        ele_dict[\"target_positions\"].append(ele[\"target_positions\"])\n",
    "        ele_dict[\"target_availabilities\"].append(ele[\"target_availabilities\"])\n",
    "        ele_dict[\"centroid\"].append(ele[\"centroid\"])  # 这个元素是为了建图方便\n",
    "        ele_dict[\"index\"].append(cur_index)  # 这个元素是为了建图方便\n",
    "\n",
    "    return_np = [] # 这个的长度应该是247或者248的样子，并且这个就是frame_index\n",
    "\n",
    "    for ele in scene_ego_dataset:\n",
    "        ele_dict = {}\n",
    "        for name in [\"x\",\"edge_index\",\"edge_attr\",\"centroid\",\"index\",\"target_positions\",\"target_availabilities\"]:\n",
    "            if name == \"edge_index\":\n",
    "                ele_dict[name] = [[],[]]\n",
    "            else:\n",
    "                ele_dict[name] = []\n",
    "        add_element_into(ele_dict,ele)\n",
    "        return_np.append(ele_dict)\n",
    "    if debug:\n",
    "        print(f\"len(return_np) : {len(return_np)}\")\n",
    "    for ele in scene_agent_dataset:\n",
    "        curr_frame_index = ele[\"frame_index\"]\n",
    "        if debug:\n",
    "            print(ele[\"frame_index\"])\n",
    "        cur_dict = return_np[curr_frame_index]\n",
    "        add_element_into(cur_dict, ele)\n",
    "\n",
    "    for ele in return_np:\n",
    "        for key_ in ele.keys():\n",
    "            ele[key_] = np.array(ele[key_])\n",
    "\n",
    "        ele[\"x\"] = torch.tensor(ele[\"x\"])\n",
    "        ele[\"edge_index\"] = torch.tensor(ele[\"edge_index\"],dtype=torch.long)\n",
    "        ele[\"edge_attr\"] = torch.tensor(ele[\"edge_attr\"])\n",
    "        ele[\"target_positions\"] = torch.tensor(ele[\"target_positions\"])\n",
    "        ele[\"target_availabilities\"] = torch.tensor(ele[\"target_availabilities\"])\n",
    "\n",
    "    return return_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare model & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:41:59.739938Z",
     "iopub.status.busy": "2022-10-13T03:41:59.739776Z",
     "iopub.status.idle": "2022-10-13T03:42:00.025419Z",
     "shell.execute_reply": "2022-10-13T03:42:00.024831Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "libcusparse.so.11: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCNConv\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGCN\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, hidden_dim, output_dim, num_layers,\n\u001b[1;32m      6\u001b[0m                  dropout, return_embeds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, whether_dropout\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# TODO: Implement this function that initializes self.convs,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# self.bns, and self.softmax.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModuleType\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_module\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/data/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhetero_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HeteroData\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemporal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TemporalData\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_geometric/data/data.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_sparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseTensor\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     FeatureStore,\n\u001b[1;32m     24\u001b[0m     FeatureTensorType,\n\u001b[1;32m     25\u001b[0m     TensorAttr,\n\u001b[1;32m     26\u001b[0m     _field_status,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     EDGE_LAYOUT_TO_ATTR_NAME,\n\u001b[1;32m     30\u001b[0m     EdgeAttr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     edge_tensor_type_to_adj_type,\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch_sparse/__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m spec \u001b[38;5;241m=\u001b[39m cuda_spec \u001b[38;5;129;01mor\u001b[39;00m cpu_spec\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibrary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mosp\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_ops.py:255\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    250\u001b[0m path \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/ctypes/__init__.py:382\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: libcusparse.so.11: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
    "                 dropout, return_embeds=False, whether_dropout= False):\n",
    "        # TODO: Implement this function that initializes self.convs,\n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        # A list of GCNConv layers\n",
    "        self.convs = None\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = None\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = None\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        self.convs.append(GCNConv(input_dim, output_dim))\n",
    "        self.whether_dropout = whether_dropout\n",
    "\n",
    "        self.softmax=torch.nn.LogSoftmax()\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "\n",
    "\n",
    "    # def forward(self, x, edge_index,edge_attr):\n",
    "    #     # x = x.to(device) # 在模型to_divice很慢？\n",
    "    #     edge_attr = edge_attr.to(device)\n",
    "    #     edge_index = edge_index.to(device)\n",
    "    #     out = None\n",
    "    #     for layer in range(len(self.convs)-1):  #layer：层数\n",
    "    #         x=self.convs[layer](x,edge_index,edge_attr).to(torch.float)   #叠GCNConv\n",
    "    #         # x= x.to(torch.float)# 这个是因为他这的输出搞成了float64,导致大家数据形式不兼容\n",
    "    #         x=F.relu(x)  #叠relu,这个不会导致数据被转化成float,\n",
    "    #         if self.whether_dropout is True:\n",
    "    #             x=F.dropout(x,self.dropout,self.training)  #叠dropout。这个self.dropout看下文是概率。\n",
    "    #     #最后一层\n",
    "    #     out=self.convs[-1](x,edge_index,edge_attr)  #GCNVonv\n",
    "    #     if not self.return_embeds:\n",
    "    #         out=self.softmax(out)\n",
    "    def forward(self, batch_data):\n",
    "        out = None\n",
    "        x = batch_data.x\n",
    "        for layer in range(len(self.convs)-1):  #layer：层数\n",
    "            x=self.convs[layer](x,batch_data.edge_index,batch_data.edge_attr).to(torch.float)   #叠GCNConv\n",
    "\n",
    "            # x= x.to(torch.float)# 这个是因为他这的输出搞成了float64,导致大家数据形式不兼容\n",
    "            x=F.relu(x)  #叠relu,这个不会导致数据被转化成float,\n",
    "            if self.whether_dropout is True:\n",
    "                x=F.dropout(x,self.dropout,self.training)  #叠dropout。这个self.dropout看下文是概率。\n",
    "        #最后一层\n",
    "\n",
    "        out=self.convs[-1](x,batch_data.edge_index,batch_data.edge_attr)  #GCNVonv\n",
    "        if not self.return_embeds:\n",
    "            out=self.softmax(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:42:00.028211Z",
     "iopub.status.busy": "2022-10-13T03:42:00.028014Z",
     "iopub.status.idle": "2022-10-13T03:42:00.031223Z",
     "shell.execute_reply": "2022-10-13T03:42:00.030740Z"
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    # 'device' = 'cpu'\n",
    "    'num_layers': 2,\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 1000,\n",
    "    'train_scene_start': 10, # 这个该有多少，还要好好想想，因为我们存储的数据并不多，主要是egodataset的load速度那边被限制住了\n",
    "    'train_scene_end': 100,\n",
    "    'test_scene_start': 10,\n",
    "    'test_scene_end': 30,\n",
    "    'batch_size_gcn':512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:42:00.034047Z",
     "iopub.status.busy": "2022-10-13T03:42:00.033799Z",
     "iopub.status.idle": "2022-10-13T03:42:00.037069Z",
     "shell.execute_reply": "2022-10-13T03:42:00.036530Z"
    }
   },
   "outputs": [],
   "source": [
    "#dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:42:00.039827Z",
     "iopub.status.busy": "2022-10-13T03:42:00.039611Z",
     "iopub.status.idle": "2022-10-13T03:42:23.609191Z",
     "shell.execute_reply": "2022-10-13T03:42:23.608697Z"
    }
   },
   "outputs": [],
   "source": [
    "scene_ego_dataset = train_ego_dataset.get_scene_dataset(60)\n",
    "scene_agent_dataset = train_agent_dataset.get_scene_dataset(60)\n",
    "frame_dic_array = transform_one_scene_dataset(scene_ego_dataset,scene_agent_dataset,debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:42:23.611668Z",
     "iopub.status.busy": "2022-10-13T03:42:23.611497Z",
     "iopub.status.idle": "2022-10-13T03:42:24.846919Z",
     "shell.execute_reply": "2022-10-13T03:42:24.846404Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm,trange\n",
    "\n",
    "input_dim = len(frame_dic_array[0][\"x\"][0])\n",
    "model_gcn = GCN(input_dim=input_dim, hidden_dim=args[\"hidden_dim\"], output_dim = cfg[\"model_params\"][\"future_num_frames\"] * 2,num_layers=args[\"num_layers\"],dropout=args[\"dropout\"],return_embeds = True, whether_dropout=False)\n",
    "\n",
    "model_gcn = model_gcn.to(device=args['device'])\n",
    "\n",
    "print(model_gcn.eval())\n",
    "epoch = flags.epoch\n",
    "\n",
    "optimizer = torch.optim.Adam(model_gcn.parameters(), lr=args[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:42:24.849426Z",
     "iopub.status.busy": "2022-10-13T03:42:24.849240Z",
     "iopub.status.idle": "2022-10-13T03:42:24.857912Z",
     "shell.execute_reply": "2022-10-13T03:42:24.857425Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_with_batch(model, device, data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    # for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            out=model(batch)\n",
    "            train_output=out.view([-1,50,2]).to(device)\n",
    "            train_label = batch.y[:,:,0:2]\n",
    "            train_availabilities = torch.squeeze(batch.y[:,:,0:1],dim=-1)\n",
    "            loss=loss_fn(train_label,train_output,train_availabilities)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def train(model, value_dic, train_idx, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out=model(torch.tensor(value_dic[\"x\"]),edge_index = torch.tensor(value_dic[\"edge_index\"],dtype=torch.long), edge_attr = torch.tensor(value_dic[\"edge_attr\"]))\n",
    "    if len(out.shape) <= 2:\n",
    "        out = torch.unsqueeze(out,0)\n",
    "\n",
    "    train_output=out.view([-1,50,2]).to(args['device'])  # 这里暂时是全部训练\n",
    "    train_label= torch.tensor(value_dic[\"target_positions\"], dtype = torch.float).to(args['device'])\n",
    "    train_availabilities = torch.tensor(value_dic[\"target_availabilities\"],dtype= torch.int).to(args['device'])\n",
    "    loss=loss_fn(train_label,train_output,train_availabilities) # 只预测一条路\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_with_batch(model, device, data_loader, loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    loss_array = []\n",
    "\n",
    "    # for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            out=model(batch)\n",
    "            train_output=out.view([-1,50,2]).to(device)\n",
    "            train_label = batch.y[:,:,0:2]\n",
    "            train_availabilities = torch.squeeze(batch.y[:,:,0:1],dim=-1)\n",
    "            loss=loss_fn(train_label,train_output,train_availabilities)\n",
    "            y_true.append(train_label.detach().cpu())\n",
    "            y_pred.append(train_output.detach().cpu())\n",
    "            loss_array.append(loss.item())\n",
    "\n",
    "    loss_array = np.array(loss_array)\n",
    "\n",
    "\n",
    "\n",
    "    return y_true,y_pred,loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:42:24.860171Z",
     "iopub.status.busy": "2022-10-13T03:42:24.860013Z",
     "iopub.status.idle": "2022-10-13T03:42:24.863191Z",
     "shell.execute_reply": "2022-10-13T03:42:24.862832Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import  DataLoader\n",
    "def convert_framearray_to_dataloader(frame_dic_array):\n",
    "    pyg_data_list = []\n",
    "    for ele in frame_dic_array:\n",
    "        pyg_data_list.append(Data(x = ele[\"x\"],edge_index=ele[\"edge_index\"],edge_attr=ele[\"edge_attr\"],y=torch.cat([ele[\"target_positions\"],torch.unsqueeze(ele[\"target_availabilities\"],dim=-1)],dim = -1)))\n",
    "    return DataLoader(pyg_data_list,batch_size=args['batch_size_gcn'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_all_data_into_memory(ego_dataset,agent_dataset,start_index,end_index):\n",
    "    return_array = []\n",
    "    for index_ in range(start_index,end_index):\n",
    "\n",
    "        scene_ego_dataset = ego_dataset.get_scene_dataset(index_)\n",
    "        scene_agent_dataset = agent_dataset.get_scene_dataset(index_)\n",
    "        frame_dic_array = transform_one_scene_dataset(scene_ego_dataset,scene_agent_dataset,debug=False)\n",
    "        dataloader = convert_framearray_to_dataloader(frame_dic_array)\n",
    "        return_array.append(dataloader)\n",
    "    return  return_array\n",
    "\n",
    "def prepare_dataloader_array():\n",
    "    train_dataloader = load_all_data_into_memory(train_ego_dataset,train_agent_dataset,args['train_scene_start'],args['train_scene_end'])\n",
    "    test_dataloader = load_all_data_into_memory(test_ego_dataset,test_agent_dataset,args['test_scene_start'],args['test_scene_end'])\n",
    "    return train_dataloader,test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataloader,test_dataloader = prepare_dataloader_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(len(train_dataloader))\n",
    "# print(sys.getsizeof(train_dataloader[0]) / 1024 / 1024, 'MB')\n",
    "# for i in trange(0,100):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T03:42:24.865235Z",
     "iopub.status.busy": "2022-10-13T03:42:24.865059Z",
     "iopub.status.idle": "2022-10-13T13:02:10.076070Z",
     "shell.execute_reply": "2022-10-13T13:02:10.075668Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_file = open( 'test_output.txt', 'w+',encoding = 'utf-8' )\n",
    "for epoch in trange(1, 1 + args[\"epochs\"]):\n",
    "    loss_whole_scene_list = []\n",
    "    loss_this_epoch = 0\n",
    "    # for train_scene_index in trange(0,args['train_scene_end']-args['train_scene_start']):\n",
    "    for train_scene_index in range(0,args['train_scene_end']-args['train_scene_start']):\n",
    "\n",
    "        dataloader = train_dataloader[train_scene_index]\n",
    "\n",
    "\n",
    "        loss_each_frame = train_with_batch(model_gcn,args[\"device\"],dataloader,optimizer,pytorch_neg_multi_log_likelihood_single)\n",
    "        loss_this_scene = np.mean(np.array(loss_each_frame))\n",
    "        loss_this_epoch = loss_this_epoch + loss_this_scene\n",
    "\n",
    "    # print(loss_this_epoch)\n",
    "    if epoch % 10 == 1:\n",
    "        loss_list = []\n",
    "        for test_scene_index in range(0,args['test_scene_end']-args['test_scene_start']):\n",
    "\n",
    "            dataloader = test_dataloader[test_scene_index]\n",
    "            y_true,y_pred,loss = test_with_batch(model_gcn,args[\"device\"],dataloader,pytorch_neg_multi_log_likelihood_single)\n",
    "            loss_list.append(loss)\n",
    "        loss_list = np.array(loss_list)\n",
    "        test_file.write(str(loss_list.mean()))\n",
    "        print(str(loss_list.mean()))\n",
    "        test_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "test_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T13:02:10.079596Z",
     "iopub.status.busy": "2022-10-13T13:02:10.079399Z",
     "iopub.status.idle": "2022-10-13T13:02:10.100590Z",
     "shell.execute_reply": "2022-10-13T13:02:10.100012Z"
    }
   },
   "outputs": [],
   "source": [
    "print(frame_dic_array[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtrain training history results really easily by just accessing `LogReport` class, which is useful for managing a lot of experiments during kaggle competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history log and model's weight are saved by \"extensions\" (`LogReport` and `E.snapshot_object` respectively) easily, which is a benefit of using training abstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T13:02:10.102975Z",
     "iopub.status.busy": "2022-10-13T13:02:10.102799Z",
     "iopub.status.idle": "2022-10-13T13:02:10.254308Z",
     "shell.execute_reply": "2022-10-13T13:02:10.253380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see training results directory\n",
    "\n",
    "!ls results/multi_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Items to try\n",
    "\n",
    "This kernel shows demonstration run of the training (`debug=True`). You can try these things to see how the score changes at first\n",
    " - set debug=False to train with actual training dataset\n",
    " - change training hyperparameters (training epoch, change optimizer, scheduler learning rate etc...)\n",
    "   - Especially, just training much longer time may improve the score.\n",
    " \n",
    "To go further, these items may be nice to try:\n",
    " - Change the cnn model (now simple resnet18 is used as baseline modeling)\n",
    " - Training the model using full dataset: [lyft-full-training-set](https://www.kaggle.com/philculliton/lyft-full-training-set)\n",
    " - Write your own rasterizer to prepare input image as motivation explained in previous kernel.\n",
    " - Consider much better scheme to predict multi-trajectory\n",
    "    - The model just predicts multiple trajectory at the same time in this kernel, but it is possible to collapse \"trivial\" solution where all trajectory converges to same. How to avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next to go\n",
    "\n",
    "[Update 2020/9/6]<br/>\n",
    "Published prediction kernel: [Lyft: Prediction with multi-mode confidence](https://www.kaggle.com/corochann/lyft-prediction-with-multi-mode-confidence)<br/>\n",
    "Try yourself how good score you can get using only single model without ensemble! :)\n",
    "\n",
    "To understand the competition in more detail, please refer my other kernels too.\n",
    " - [Lyft: Comprehensive guide to start competition](https://www.kaggle.com/corochann/lyft-comprehensive-guide-to-start-competition)\n",
    " - [Lyft: Deep into the l5kit library](https://www.kaggle.com/corochann/lyft-deep-into-the-l5kit-library)\n",
    " - [Save your time, submit without kernel inference](https://www.kaggle.com/corochann/save-your-time-submit-without-kernel-inference)\n",
    " - [Lyft: pytorch implementation of evaluation metric](https://www.kaggle.com/corochann/lyft-pytorch-implementation-of-evaluation-metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reference\n",
    "\n",
    " - Paper of this Lyft Level 5 prediction dataset: [One Thousand and One Hours: Self-driving Motion Prediction Dataset](https://arxiv.org/abs/2006.14480)\n",
    " - [jpbremer/lyft-scene-visualisations](https://www.kaggle.com/jpbremer/lyft-scene-visualisations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\">If this kernel helps you, please upvote to keep me motivated :)<br>Thanks!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyft: Training with multi-mode confidence\n",
    "\n",
    "![](http://www.l5kit.org/_images/av.jpg)\n",
    "<cite>The image from L5Kit official document: <a href=\"http://www.l5kit.org/README.html\">http://www.l5kit.org/README.html</a></cite>\n",
    "\n",
    "Continued from the previous kernel:\n",
    " - [Lyft: Comprehensive guide to start competition](https://www.kaggle.com/corochann/lyft-comprehensive-guide-to-start-competition)\n",
    " - [Lyft: Deep into the l5kit library](https://www.kaggle.com/corochann/lyft-deep-into-the-l5kit-library)\n",
    "\n",
    "In this kernel, I will run **pytorch CNN model training**. Especially, followings are new items to try:\n",
    " - Predict **multi-mode with confidence**: As written in [evaluation metric](https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/overview/evaluation) page, we can predict **3 modes** of motion trajectory.\n",
    " - Training loss with **competition evaluation metric**\n",
    " - Use Training abstraction library **`pytorch-ignite` and `pytorch-pfn-extras`**.\n",
    "\n",
    "\n",
    "[Update 2020/9/6]<br/>\n",
    "Published prediction kernel: [Lyft: Prediction with multi-mode confidence](https://www.kaggle.com/corochann/lyft-prediction-with-multi-mode-confidence)<br/>\n",
    "Try yourself how good score you can get using only single model without ensemble! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "\n",
    " - Please add [pestipeti/lyft-l5kit-unofficial-fix](https://www.kaggle.com/pestipeti/lyft-l5kit-unofficial-fix) as utility script.\n",
    "    - Official utility script \"[philculliton/kaggle-l5kit](https://www.kaggle.com/mathurinache/kaggle-l5kit)\" does not work with pytorch GPU.\n",
    " - Please add [lyft-config-files](https://www.kaggle.com/jpbremer/lyft-config-files) as dataset\n",
    " \n",
    "See previous kernel [Lyft: Comprehensive guide to start competition](https://www.kaggle.com/corochann/lyft-comprehensive-guide-to-start-competition) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pfn-extras==0.6.1 in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (0.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.10 in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-pfn-extras==0.6.1) (4.3.0)\r\n",
      "Requirement already satisfied: numpy in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-pfn-extras==0.6.1) (1.19.5)\r\n",
      "Requirement already satisfied: packaging in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-pfn-extras==0.6.1) (21.3)\r\n",
      "Requirement already satisfied: torch in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-pfn-extras==0.6.1) (1.12.1+cu113)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from packaging->pytorch-pfn-extras==0.6.1) (3.0.9)\r\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pfnet/pytorch-pfn-extras/releases/tag/v0.3.1\n",
    "!pip install pytorch-pfn-extras==0.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361104/525924121.py:16: DeprecationWarning:\n",
      "\n",
      "Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "# pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l5kit version: 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "import l5kit\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "print(\"l5kit version:\", l5kit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import pytorch_pfn_extras as ppe\n",
    "from math import ceil\n",
    "from pytorch_pfn_extras.training import IgniteExtensionsManager\n",
    "from pytorch_pfn_extras.training.triggers import MinValueTrigger\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "import pytorch_pfn_extras.training.extensions as E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"upgrea\"\r\n"
     ]
    }
   ],
   "source": [
    "!pip upgrea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# --- Dataset utils ---\n",
    "from typing import Callable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset, transform: Callable):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.dataset[index]\n",
    "        return self.transform(batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "To define loss function to calculate competition evaluation metric **in batch**.<br/>\n",
    "It works with **pytorch tensor, so it is differentiable** and can be used for training Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# --- Function utils ---\n",
    "# Original code from https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/metrics.py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute a negative log-likelihood for the multi-modal scenario.\n",
    "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    https://leimao.github.io/blog/LogSumExp/\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
    "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_single(\n",
    "    gt: Tensor, pred: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
    "    # create confidence (bs)x(mode=1)\n",
    "    batch_size, future_len, num_coords = pred.shape\n",
    "    confidences = pred.new_ones((batch_size, 1))\n",
    "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "pytorch model definition. Here model outputs both **multi-mode trajectory prediction & confidence of each trajectory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "# --- Model utils ---\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "class LyftMultiModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: Dict, num_modes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: support other than resnet18?\n",
    "        backbone = resnet18(pretrained=True, progress=True)\n",
    "        self.backbone = backbone\n",
    "\n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # This is 512 for resnet18 and resnet34;\n",
    "        # And it is 2048 for the other resnets\n",
    "        backbone_out_features = 512\n",
    "\n",
    "        # X, Y coords for the future positions (output shape: Bx50x2)\n",
    "        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        num_targets = 2 * self.future_len\n",
    "        2*+3\n",
    "\n",
    "        # You can add more layers here.\n",
    "        self.head = nn.Sequential(\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "        )\n",
    "\n",
    "        self.num_preds = num_targets * num_modes\n",
    "        self.num_modes = num_modes\n",
    "\n",
    "        self.logit = nn.Linear(4096, out_features=self.num_preds + num_modes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.head(x) #512-->4096\n",
    "        x = self.logit(x) #4096---> num_targets * num_modes+ num_modes\n",
    "\n",
    "        # pred (bs)x(modes)x(time)x(2D coords)\n",
    "        # confidences (bs)x(modes)\n",
    "        print('x:{}'.format(x))\n",
    "        print('x.shape:{}'.format(x.shape))\n",
    "\n",
    "\n",
    "        bs, _ = x.shape\n",
    "        # [12,303]\n",
    "        # []\n",
    "        pred, confidences = torch.split(x, self.num_preds, dim=1)\n",
    "        pred = pred.view(bs, self.num_modes, self.future_len, 2)\n",
    "        assert confidences.shape == (bs, self.num_modes)\n",
    "        confidences = torch.softmax(confidences, dim=1)\n",
    "        return pred, confidences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_trajectory(dataset, index, title=\"target_positions movement with draw_trajectory\"):\n",
    "    data = dataset[index]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "    draw_trajectory(im, target_positions_pixels, yaws=data[\"target_yaws\"], rgb_color=TARGET_POINTS_COLOR)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.imshow(im[::-1])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftMultiRegressor(nn.Module):\n",
    "    \"\"\"Single mode prediction\"\"\"\n",
    "\n",
    "    def __init__(self, predictor, lossfun=pytorch_neg_multi_log_likelihood_batch):\n",
    "        super().__init__()\n",
    "        self.predictor = predictor\n",
    "        self.lossfun = lossfun\n",
    "\n",
    "    def forward(self, image, targets, target_availabilities):\n",
    "        pred, confidences = self.predictor(image)\n",
    "        loss = self.lossfun(targets, pred, confidences, target_availabilities)\n",
    "        metrics = {\n",
    "            \"loss\": loss.item(),\n",
    "            \"nll\": pytorch_neg_multi_log_likelihood_batch(targets, pred, confidences, target_availabilities).item()\n",
    "        }\n",
    "        ppe.reporting.report(metrics, self)\n",
    "        return loss, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Ignite\n",
    "\n",
    "I use [pytorch-ignite](https://github.com/pytorch/ignite) for training abstraction.<br/>\n",
    "`Engine` defines the 1 iteration of training update.\n",
    "\n",
    " - [Official Document: IGNITE YOUR NETWORKS!](https://pytorch.org/ignite/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# --- Training utils ---\n",
    "from ignite.engine import Engine\n",
    "\n",
    "\n",
    "def create_trainer(model, optimizer, device) -> Engine:\n",
    "    model.to(device)\n",
    "\n",
    "    def update_fn(engine, batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss, metrics = model(*[elem.to(device) for elem in batch])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return metrics\n",
    "    trainer = Engine(update_fn)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# --- Utils ---\n",
    "import yaml\n",
    "def save_yaml(filepath, content, width=120):\n",
    "    with open(filepath, 'w') as f:\n",
    "        yaml.dump(content, f, width=width)\n",
    "def load_yaml(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    return content\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "\n",
    "    Refer: https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary/23689767#23689767\n",
    "    \"\"\"  # NOQA\n",
    "\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# --- Lyft configs ---\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet50',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'render_ego_history':True,\n",
    "        'step_time':0.1\n",
    "    },\n",
    "\n",
    "    'raster_params': {\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5,\n",
    "\n",
    "        'set_origin_to_bottom': True,\n",
    "        'disable_traffic_light_faces':False\n",
    "    },\n",
    "\n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 12,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'valid_data_loader': {\n",
    "        'key': 'scenes/validate.zarr',\n",
    "        'batch_size': 32,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'train_params': {\n",
    "        'max_num_steps': 10000,\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "\n",
    "        # 'eval_every_n_steps': -1\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "flags_dict = {\n",
    "    \"debug\": True,\n",
    "    # --- Data configs ---\n",
    "    \"l5kit_data_folder\": \"/home/zhuhe/kaggle/input/lyft-motion-prediction-autonomous-vehicles\",\n",
    "    # --- Model configs ---\n",
    "    \"pred_mode\": \"multi\",\n",
    "    # --- Training configs ---\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"out_dir\": \"results/multi_train\",\n",
    "    \"epoch\": 2,\n",
    "    \"snapshot_freq\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script\n",
    "\n",
    "Now finished defining all the util codes. Let's start writing main script to train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Here we will only use the first dataset from the sample set. (sample.zarr data is used for visualization, please use train.zarr / validate.zarr / test.zarr for actual model training/validation/prediction.)<br/>\n",
    "We're building a `LocalDataManager` object. This will resolve relative paths from the config using the `L5KIT_DATA_FOLDER` env variable we have just set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flags: {'debug': True, 'l5kit_data_folder': '/home/zhuhe/kaggle/input/lyft-motion-prediction-autonomous-vehicles', 'pred_mode': 'multi', 'device': 'cuda:0', 'out_dir': 'results/multi_train', 'epoch': 2, 'snapshot_freq': 50}\n"
     ]
    }
   ],
   "source": [
    "flags = DotDict(flags_dict)\n",
    "out_dir = Path(flags.out_dir)\n",
    "os.makedirs(str(out_dir), exist_ok=True)\n",
    "print(f\"flags: {flags_dict}\")\n",
    "save_yaml(out_dir / 'flags.yaml', flags_dict)\n",
    "save_yaml(out_dir / 'cfg.yaml', cfg)\n",
    "debug = flags.debug\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset...\n",
      "train_zarr <class 'l5kit.data.zarr_dataset.ChunkedDataset'>\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "valid_zarr <class 'l5kit.data.zarr_dataset.ChunkedDataset'>\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "# AgentDataset train: 111634 #valid 111634\n",
      "# ActualDataset train: 1000 #valid 100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = flags.l5kit_data_folder\n",
    "dm = LocalDataManager(None)\n",
    "\n",
    "print(\"Load dataset...\")\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "valid_cfg = cfg[\"valid_data_loader\"]\n",
    "\n",
    "# 'key': 'scenes/train.zarr',\n",
    "# 'key': 'scenes/validate.zarr',\n",
    "\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "\n",
    "\n",
    "# Train dataset/dataloader\n",
    "def transform(batch):\n",
    "    return batch[\"image\"], batch[\"target_positions\"], batch[\"target_availabilities\"]\n",
    "\n",
    "\n",
    "train_path = \"scenes/sample.zarr\" if debug else train_cfg[\"key\"]\n",
    "train_zarr = ChunkedDataset(dm.require(train_path)).open()\n",
    "print(\"train_zarr\", type(train_zarr))\n",
    "train_agent_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataset = TransformDataset(train_agent_dataset, transform)\n",
    "if debug:\n",
    "    # Only use 1000 dataset for fast check...\n",
    "    train_dataset = Subset(train_dataset, np.arange(1000))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=train_cfg[\"shuffle\"],\n",
    "                          batch_size=train_cfg[\"batch_size\"],\n",
    "                          num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_agent_dataset)\n",
    "\n",
    "valid_path = \"scenes/sample.zarr\" if debug else valid_cfg[\"key\"]\n",
    "valid_zarr = ChunkedDataset(dm.require(valid_path)).open()\n",
    "print(\"valid_zarr\", type(train_zarr))\n",
    "valid_agent_dataset = AgentDataset(cfg, valid_zarr, rasterizer)\n",
    "valid_dataset = TransformDataset(valid_agent_dataset, transform)\n",
    "if debug:\n",
    "    # Only use 100 dataset for fast check...\n",
    "    valid_dataset = Subset(valid_dataset, np.arange(100))\n",
    "else:\n",
    "    # Only use 1000 dataset for fast check...\n",
    "    valid_dataset = Subset(valid_dataset, np.arange(1000))\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    shuffle=valid_cfg[\"shuffle\"],\n",
    "    batch_size=valid_cfg[\"batch_size\"],\n",
    "    num_workers=valid_cfg[\"num_workers\"]\n",
    ")\n",
    "\n",
    "print(valid_agent_dataset)\n",
    "print(\"# AgentDataset train:\", len(train_agent_dataset), \"#valid\", len(valid_agent_dataset))\n",
    "print(\"# ActualDataset train:\", len(train_dataset), \"#valid\", len(valid_dataset))\n",
    "# AgentDataset train: 22496709 #valid 21624612\n",
    "# ActualDataset train: 100 #valid 100\n",
    "\n",
    "\n",
    "visualize_trajectory(train_agent_dataset, index=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 224, 224)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agent_dataset[0]['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning:\n",
      "\n",
      "The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "\n",
      "/home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning:\n",
      "\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(flags.device)\n",
    "\n",
    "if flags.pred_mode == \"multi\":\n",
    "    predictor = LyftMultiModel(cfg)\n",
    "    model = LyftMultiRegressor(predictor)\n",
    "else:\n",
    "    raise ValueError(f\"[ERROR] Unexpected value flags.pred_mode={flags.pred_mode}\")\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write training code\n",
    "\n",
    "pytorch-ignite & pytorch-pfn-extras are used here.\n",
    "\n",
    " - [pytorch/ignite](https://github.com/pytorch/ignite): It provides abstraction for writing training loop.\n",
    " - [pfnet/pytorch-pfn-extras](https://github.com/pfnet/pytorch-pfn-extras): It provides several \"extensions\" useful for training. Useful for **logging, printing, evaluating, saving the model, scheduling the learning rate** during training.\n",
    " \n",
    "**[Note] Why training abstraction library is used?**\n",
    "\n",
    "You may feel understanding training abstraction code below is a bit unintuitive compared to writing \"raw\" training loop.<br/>\n",
    "The advantage of abstracting the code is that we can re-use implemented handler class for other training, other competition.<br/>\n",
    "You don't need to write code for saving models, logging training loss/metric, show progressbar etc.\n",
    "These are done by provided util classes in `pytorch-pfn-extras` library!\n",
    "\n",
    "You may refer my other kernel in previous competition too: [Bengali: SEResNeXt training with pytorch](https://www.kaggle.com/corochann/bengali-seresnext-training-with-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-ignite in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (0.4.10)\r\n",
      "Requirement already satisfied: torch<2,>=1.3 in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-ignite) (1.12.1+cu113)\r\n",
      "Requirement already satisfied: packaging in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from pytorch-ignite) (21.3)\r\n",
      "Requirement already satisfied: typing-extensions in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from torch<2,>=1.3->pytorch-ignite) (4.3.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/zhuhe/anaconda3/envs/pytorch/lib/python3.9/site-packages (from packaging->pytorch-ignite) (3.0.9)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pytorch-ignite --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6d06b9e41e4a32aa105dde6db6a224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatProgress(value=0.0, bar_style='info', description='total', max=1.0), HTML(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d04940eb1954bf397853beb46614718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 0.2754, -0.2857,  0.2318,  ..., -0.3443, -0.2202, -0.1376],\n",
      "        [-0.0855, -0.4850,  0.1953,  ..., -0.1107,  0.0144, -0.8450],\n",
      "        [-0.0811, -0.3050,  0.2189,  ..., -0.0400, -0.0442, -0.8543],\n",
      "        ...,\n",
      "        [ 0.2895, -0.6930,  0.5169,  ..., -0.1595, -0.2694, -0.4254],\n",
      "        [ 0.1866, -0.4870,  0.3744,  ..., -0.0432, -0.0584, -0.4967],\n",
      "        [ 0.4564, -0.2592,  0.1672,  ..., -0.3367, -0.1906, -0.3099]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 1.9408,  1.2903,  2.6239,  ...,  1.3413, -1.1330, -1.1343],\n",
      "        [ 1.7895,  1.2678,  2.3488,  ...,  1.1422, -1.4987, -0.8378],\n",
      "        [ 2.5499,  1.7812,  2.4228,  ...,  1.1660, -1.4219, -0.9872],\n",
      "        ...,\n",
      "        [ 1.6136,  1.3647,  1.7319,  ...,  0.8479, -1.5335, -0.3438],\n",
      "        [ 1.7356,  1.0790,  2.4216,  ...,  1.4641, -1.0971, -0.9907],\n",
      "        [ 1.6728,  1.2138,  1.7639,  ...,  0.7784, -1.2856, -0.3452]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 0.4810, -0.2169,  2.9719,  ..., -0.6717,  0.0102,  1.5377],\n",
      "        [ 0.7039, -0.8754,  4.4080,  ..., -1.7686,  0.3924,  1.7453],\n",
      "        [ 0.7838, -0.5102,  4.4038,  ..., -1.8876,  0.5234,  1.5576],\n",
      "        ...,\n",
      "        [ 0.4005, -0.1620,  1.3563,  ..., -0.6799, -0.1133,  0.7131],\n",
      "        [ 0.6173, -0.3132,  2.4426,  ..., -0.9765, -0.1835,  1.3022],\n",
      "        [ 0.3949, -0.2378,  1.7833,  ..., -0.7456,  0.1040,  0.9827]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-0.6482, -1.6093,  2.1153,  ..., -2.4948,  2.0576,  1.7814],\n",
      "        [-0.7045, -1.7807,  1.2954,  ..., -1.8389,  1.2310,  1.7037],\n",
      "        [-0.6653, -1.5810,  2.1972,  ..., -2.5686,  1.7334,  1.8628],\n",
      "        ...,\n",
      "        [-1.8717, -4.8356,  5.6362,  ..., -6.7294,  5.4523,  4.4876],\n",
      "        [-0.3906, -1.2634,  1.3744,  ..., -1.7271,  1.0084,  1.4884],\n",
      "        [-0.3808, -1.3851,  1.6619,  ..., -1.7991,  1.2689,  1.6612]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  0.5710,  -5.9339,   8.9407,  ..., -14.7191,   4.3198,  14.7868],\n",
      "        [ -0.0990,  -2.8726,   4.3366,  ...,  -8.2367,   2.8360,   8.1076],\n",
      "        [  0.3434,  -0.7509,   1.2014,  ...,  -2.3785,   0.5080,   2.7464],\n",
      "        ...,\n",
      "        [  0.1858,  -2.1350,   3.1666,  ...,  -6.5458,   1.8288,   6.3211],\n",
      "        [  0.3470,  -0.7941,   1.3542,  ...,  -2.6348,   0.5800,   2.9349],\n",
      "        [  0.0663,  -3.2872,   5.2050,  ...,  -9.0723,   2.3212,   9.2250]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  0.6041,  -0.2360,   0.8197,  ...,  -2.4196,   0.3497,   2.2679],\n",
      "        [  2.1870,  -1.7287,   4.1699,  ..., -11.0566,   3.4814,   9.2667],\n",
      "        [  1.7779,  -1.6620,   3.5970,  ...,  -9.4782,   3.0406,   8.6183],\n",
      "        ...,\n",
      "        [  0.6413,  -0.3283,   0.9787,  ...,  -2.7135,   0.4852,   2.5813],\n",
      "        [  0.5774,  -0.2509,   0.8046,  ...,  -2.4262,   0.3711,   2.3636],\n",
      "        [  2.0537,  -1.7266,   4.1226,  ..., -10.4002,   3.6074,   9.1994]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 1.0592e+00, -1.3956e-01,  9.0528e-01,  ..., -4.2979e+00,\n",
      "          1.4184e+00,  2.7437e+00],\n",
      "        [ 8.5223e-01, -4.7369e-02,  5.0671e-01,  ..., -2.5946e+00,\n",
      "          8.3418e-01,  1.9277e+00],\n",
      "        [ 7.3267e-01,  2.5551e-02,  3.8609e-01,  ..., -2.1531e+00,\n",
      "          5.5207e-01,  1.6094e+00],\n",
      "        ...,\n",
      "        [ 7.9151e-01, -1.2724e-02,  4.8924e-01,  ..., -2.4986e+00,\n",
      "          7.7212e-01,  1.8346e+00],\n",
      "        [ 7.5862e-01, -6.5394e-05,  4.3890e-01,  ..., -2.2649e+00,\n",
      "          6.9061e-01,  1.7069e+00],\n",
      "        [ 1.0472e+00, -1.8854e-01,  9.0339e-01,  ..., -4.2343e+00,\n",
      "          1.6005e+00,  2.7349e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 10.0813,   1.2935,   6.3249,  ..., -28.1297,  16.0258,  12.8580],\n",
      "        [ 10.5711,   1.5689,   6.6830,  ..., -29.2884,  16.7605,  13.2610],\n",
      "        [  8.2201,   1.1814,   4.8564,  ..., -22.7941,  12.5609,  10.5106],\n",
      "        ...,\n",
      "        [  2.0601,   0.4456,   1.0114,  ...,  -5.2673,   2.3054,   2.8694],\n",
      "        [  2.3177,   0.4032,   1.0701,  ...,  -6.1840,   3.0062,   3.3490],\n",
      "        [  1.7150,   0.3540,   0.7827,  ...,  -4.3241,   1.9646,   2.3661]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  5.1922,   1.7165,   1.6754,  ..., -12.4003,   3.7560,   8.1056],\n",
      "        [  5.5814,   1.7750,   1.7898,  ..., -13.2386,   4.0469,   8.5819],\n",
      "        [  2.8120,   1.0022,   0.8180,  ...,  -6.9925,   1.7632,   4.6445],\n",
      "        ...,\n",
      "        [  5.6472,   1.7922,   1.9495,  ..., -13.3408,   4.0713,   8.6479],\n",
      "        [  1.2297,   0.3492,   0.5041,  ...,  -4.4533,   1.2901,   2.5247],\n",
      "        [  4.4521,   1.4918,   1.3158,  ..., -11.4495,   3.7425,   7.1257]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  4.0476,   1.5498,   0.7535,  ...,  -9.2914,   1.1526,   7.0586],\n",
      "        [ 10.1215,   3.7681,   2.3356,  ..., -21.9147,   3.1011,  16.5305],\n",
      "        [  3.8290,   1.5897,   0.5589,  ...,  -8.5794,   1.1535,   6.6067],\n",
      "        ...,\n",
      "        [  3.3661,   1.4534,   0.5114,  ...,  -7.5855,   0.8099,   5.8831],\n",
      "        [  3.7039,   1.5378,   0.5874,  ...,  -8.2949,   0.9992,   6.4225],\n",
      "        [  3.6043,   1.4983,   0.6003,  ...,  -8.0915,   0.9251,   6.3226]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 2.8020,  1.3289,  0.2670,  ..., -6.9492,  0.2191,  5.6509],\n",
      "        [ 3.2381,  1.5168,  0.3352,  ..., -7.7862,  0.2183,  6.3912],\n",
      "        [ 2.9030,  1.3012,  0.0925,  ..., -5.7500, -0.3302,  5.2300],\n",
      "        ...,\n",
      "        [ 2.8306,  1.2532,  0.1549,  ..., -5.6685, -0.3462,  5.0914],\n",
      "        [ 2.7811,  1.3167,  0.1779,  ..., -5.6193, -0.2389,  4.9915],\n",
      "        [ 3.3567,  1.5906,  0.3058,  ..., -8.1089,  0.2526,  6.6442]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  2.6959,   1.2365,   0.1306,  ...,  -5.3290,  -0.6562,   4.9064],\n",
      "        [  2.8209,   1.3295,   0.2051,  ...,  -5.5852,  -0.6967,   5.1063],\n",
      "        [  1.6217,   0.8360,   0.1584,  ...,  -4.0780,  -0.1764,   3.3574],\n",
      "        ...,\n",
      "        [ 22.5250,  11.3659,   1.7438,  ..., -44.4021,  -2.2308,  39.8390],\n",
      "        [  2.7952,   1.2969,   0.1507,  ...,  -5.5186,  -0.7019,   5.0279],\n",
      "        [  2.6481,   1.2500,   0.1395,  ...,  -5.1759,  -0.6508,   4.7524]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 23.1716,  12.5951,   1.0580,  ..., -43.0599,  -4.8966,  40.6029],\n",
      "        [  2.5881,   1.2749,   0.1444,  ...,  -4.9711,  -0.8476,   4.6071],\n",
      "        [ 15.0046,   8.0195,   1.0120,  ..., -27.8909,  -3.2601,  25.9095],\n",
      "        ...,\n",
      "        [  1.1546,   0.5973,   0.1728,  ...,  -2.6955,  -0.3014,   2.2140],\n",
      "        [  2.7469,   1.3661,   0.1760,  ...,  -5.2727,  -0.8602,   4.8561],\n",
      "        [  5.8396,   3.1190,   0.1481,  ..., -11.4654,  -1.3406,  10.2226]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  1.1015,   0.5688,   0.2048,  ...,  -2.3950,  -0.3846,   1.9191],\n",
      "        [  2.3765,   1.2031,   0.1892,  ...,  -4.4163,  -0.8985,   4.0438],\n",
      "        [  1.0951,   0.5721,   0.1752,  ...,  -2.3306,  -0.3972,   1.9069],\n",
      "        ...,\n",
      "        [ 12.6839,   7.1022,   0.3900,  ..., -22.8111,  -3.4778,  21.6153],\n",
      "        [  2.3311,   1.1596,   0.1853,  ...,  -4.3350,  -0.8772,   3.9517],\n",
      "        [  2.3189,   1.1491,   0.1668,  ...,  -4.3439,  -0.8482,   3.9490]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  3.0787,   1.5703,   0.2894,  ...,  -5.6005,  -0.7253,   4.5561],\n",
      "        [  3.2637,   1.8697,   0.2148,  ...,  -6.0341,  -0.6457,   4.9626],\n",
      "        [ 14.6927,   8.7190,   0.7767,  ..., -25.3995,  -2.8974,  22.4250],\n",
      "        ...,\n",
      "        [  5.5970,   3.2603,   0.6144,  ...,  -9.6430,  -1.2859,   8.3645],\n",
      "        [  1.3615,   0.7183,   0.1988,  ...,  -2.6710,  -0.2530,   1.9866],\n",
      "        [  6.5739,   3.8235,   0.1200,  ..., -11.3923,  -1.3365,  10.1904]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 1.2579e+00,  6.3535e-01,  2.7616e-01,  ..., -2.4230e+00,\n",
      "         -1.4157e-02,  1.4277e+00],\n",
      "        [ 1.1637e+00,  5.9461e-01,  2.6753e-01,  ..., -2.0241e+00,\n",
      "         -6.5846e-02,  1.2493e+00],\n",
      "        [ 2.6960e+00,  1.4412e+00,  4.4991e-01,  ..., -4.7609e+00,\n",
      "         -2.1402e-01,  3.2862e+00],\n",
      "        ...,\n",
      "        [ 1.7117e+01,  1.0151e+01,  1.9917e+00,  ..., -2.8473e+01,\n",
      "         -5.0322e-01,  2.1575e+01],\n",
      "        [ 2.3597e+01,  1.4271e+01,  2.1624e+00,  ..., -3.8734e+01,\n",
      "         -1.0294e+00,  2.9827e+01],\n",
      "        [ 2.6985e+00,  1.4151e+00,  4.0271e-01,  ..., -4.7454e+00,\n",
      "         -1.9489e-01,  3.2608e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  2.1285,   0.9481,   1.3015,  ...,  -4.6699,   0.3278,   2.7875],\n",
      "        [ 21.4184,  11.2757,  11.2893,  ..., -43.8439,   3.0609,  30.0357],\n",
      "        [  2.0315,   0.8975,   1.2490,  ...,  -4.4795,   0.3140,   2.6844],\n",
      "        ...,\n",
      "        [  2.1917,   0.9409,   1.2437,  ...,  -4.8413,   0.3795,   2.9239],\n",
      "        [  2.0942,   0.9254,   1.2731,  ...,  -4.6732,   0.3569,   2.7896],\n",
      "        [  2.1557,   0.9881,   1.2669,  ...,  -4.6978,   0.3084,   2.8243]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  0.9862,   0.2102,   1.4058,  ...,  -3.2018,   0.6101,   1.3718],\n",
      "        [  0.5946,   0.1415,   0.9518,  ...,  -1.9012,   0.3772,   0.6527],\n",
      "        [  0.9595,   0.2088,   1.4746,  ...,  -3.1872,   0.5828,   1.3277],\n",
      "        ...,\n",
      "        [  8.1588,   3.0674,  10.9356,  ..., -22.8349,   3.6988,  13.7417],\n",
      "        [  0.5988,   0.1402,   0.9743,  ...,  -1.9290,   0.4282,   0.6969],\n",
      "        [  0.9810,   0.2375,   1.4896,  ...,  -3.2417,   0.5853,   1.3655]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.9303e-01, -6.1149e-02,  1.7470e+00,  ..., -2.6942e+00,\n",
      "          6.7194e-01,  8.0333e-01],\n",
      "        [ 3.8495e+00, -3.8871e-01,  1.4879e+01,  ..., -2.0131e+01,\n",
      "          5.3175e+00,  1.0386e+01],\n",
      "        [ 5.8889e-01, -7.4845e-02,  1.8717e+00,  ..., -3.1487e+00,\n",
      "          8.5104e-01,  1.1178e+00],\n",
      "        ...,\n",
      "        [ 6.4142e-01, -8.5615e-03,  2.0018e+00,  ..., -3.4267e+00,\n",
      "          9.6643e-01,  1.1015e+00],\n",
      "        [ 5.8106e-01, -7.2359e-02,  1.8785e+00,  ..., -3.1382e+00,\n",
      "          8.4602e-01,  1.1080e+00],\n",
      "        [ 3.4765e+00, -3.5274e-01,  1.3416e+01,  ..., -1.8115e+01,\n",
      "          4.8281e+00,  9.3407e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  0.2426,  -0.2730,   2.1367,  ...,  -2.8668,   0.8542,   0.8561],\n",
      "        [  0.2949,  -0.2925,   2.6517,  ...,  -3.7723,   1.1998,   1.1070],\n",
      "        [  0.2099,  -0.2890,   2.1825,  ...,  -2.9005,   0.8753,   0.8291],\n",
      "        ...,\n",
      "        [  0.2812,  -3.9162,  20.1305,  ..., -19.3348,   6.6828,   8.7019],\n",
      "        [  0.2363,  -0.2742,   2.2249,  ...,  -2.9639,   0.8626,   0.9152],\n",
      "        [  0.3189,  -0.3080,   2.4923,  ...,  -3.3826,   1.0159,   1.0321]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-2.0351e-02, -5.1784e-01,  2.6900e+00,  ..., -3.2040e+00,\n",
      "          8.4349e-01,  1.0950e+00],\n",
      "        [-2.0285e-02, -5.3975e-01,  2.6900e+00,  ..., -3.2662e+00,\n",
      "          8.9010e-01,  1.1336e+00],\n",
      "        [ 1.6018e-02, -6.1750e-01,  3.3776e+00,  ..., -4.1357e+00,\n",
      "          1.1152e+00,  1.4839e+00],\n",
      "        ...,\n",
      "        [-3.2941e+00, -7.4648e+00,  2.5670e+01,  ..., -1.8819e+01,\n",
      "          6.9864e+00,  8.0124e+00],\n",
      "        [-7.4429e-02, -6.6859e-01,  3.3589e+00,  ..., -3.9747e+00,\n",
      "          1.0728e+00,  1.4264e+00],\n",
      "        [-2.6955e-02, -4.6779e-01,  2.5007e+00,  ..., -2.8914e+00,\n",
      "          7.3011e-01,  9.1948e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ -0.3642,  -1.0139,   4.1101,  ...,  -4.6566,   0.9027,   2.0757],\n",
      "        [ -0.2871,  -0.7439,   3.0451,  ...,  -3.3492,   0.6013,   1.4222],\n",
      "        [ -5.2142,  -7.9836,  20.4082,  ..., -11.4738,   3.8166,   5.3894],\n",
      "        ...,\n",
      "        [ -0.2944,  -0.7973,   3.2123,  ...,  -3.5457,   0.6315,   1.5470],\n",
      "        [ -0.2657,  -0.7321,   3.0332,  ...,  -3.3483,   0.5622,   1.4415],\n",
      "        [ -0.3959,  -0.9700,   4.0395,  ...,  -4.5020,   0.8534,   1.9628]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-1.0125, -1.6586,  4.9867,  ..., -4.2244,  0.2586,  2.4647],\n",
      "        [-0.5959, -1.0449,  3.1702,  ..., -2.7766,  0.1231,  1.5553],\n",
      "        [-0.7608, -1.3623,  4.4984,  ..., -4.5676,  0.3740,  2.5382],\n",
      "        ...,\n",
      "        [-1.5521, -2.4367,  6.8148,  ..., -4.9798,  0.3483,  3.0410],\n",
      "        [-0.5199, -1.0030,  3.2643,  ..., -3.1705,  0.2071,  1.7988],\n",
      "        [-0.6766, -1.2435,  4.0261,  ..., -3.8352,  0.2191,  2.1726]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ -6.9582,  -8.7491,  18.8276,  ...,  -7.0448,  -0.2969,   6.0483],\n",
      "        [ -5.9850,  -7.6873,  16.9065,  ...,  -6.5702,  -0.3304,   5.7750],\n",
      "        [ -8.0214, -10.0600,  21.6026,  ...,  -7.9037,  -0.3314,   6.8529],\n",
      "        ...,\n",
      "        [ -0.6061,  -0.9760,   3.3027,  ...,  -3.8839,  -0.1753,   2.4549],\n",
      "        [ -0.5775,  -0.8301,   3.0117,  ...,  -3.6625,  -0.1221,   2.2282],\n",
      "        [ -1.7714,  -2.3334,   5.0482,  ...,  -2.5751,  -0.1388,   1.9148]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-0.5196, -1.0780,  3.4337,  ..., -3.5835, -0.3735,  2.7905],\n",
      "        [-0.6272, -1.2017,  3.6697,  ..., -3.6322, -0.4208,  2.8793],\n",
      "        [-0.6533, -1.1622,  3.5192,  ..., -3.3083, -0.4454,  2.6162],\n",
      "        ...,\n",
      "        [-0.3296, -0.7600,  2.5364,  ..., -2.8741, -0.2756,  2.1127],\n",
      "        [-0.5159, -0.8148,  2.3962,  ..., -2.4448, -0.2174,  1.7520],\n",
      "        [-0.5553, -1.1096,  3.4816,  ..., -3.5897, -0.3927,  2.8219]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-12.7652, -16.8883,  31.7364,  ...,  -6.3367,  -1.0439,   8.0498],\n",
      "        [ -0.0665,  -0.7798,   3.3701,  ...,  -3.2623,  -0.3999,   2.7496],\n",
      "        [ -3.4692,  -5.0179,  10.3677,  ...,  -3.4940,  -0.4323,   3.6916],\n",
      "        ...,\n",
      "        [ -0.1740,  -0.9880,   3.7862,  ...,  -3.4540,  -0.4234,   2.9618],\n",
      "        [ -0.1459,  -0.9077,   3.6417,  ...,  -3.3077,  -0.4766,   2.8387],\n",
      "        [ -0.2266,  -1.0892,   4.3028,  ...,  -4.0213,  -0.3992,   3.4134]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ -5.6443,  -9.3234,  20.5551,  ...,  -7.4469,  -0.3385,   7.4774],\n",
      "        [ -8.1028, -10.9837,  20.7878,  ...,  -4.1861,  -0.1857,   5.0442],\n",
      "        [ -0.3605,  -1.9904,   6.5596,  ...,  -4.4330,  -0.2084,   3.8653],\n",
      "        ...,\n",
      "        [ -0.3655,  -1.9888,   6.8855,  ...,  -4.4525,  -0.2741,   3.9883],\n",
      "        [ -0.2241,  -1.8390,   6.2960,  ...,  -4.5013,  -0.1867,   3.8839],\n",
      "        [ -2.2912,  -5.3404,  14.7119,  ...,  -8.2002,  -0.4320,   7.4376]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.9251e-01, -7.2406e-01,  3.9168e+00,  ..., -2.9652e+00,\n",
      "         -6.9155e-03,  2.5507e+00],\n",
      "        [ 4.0595e-01, -9.1448e-01,  4.0754e+00,  ..., -3.0209e+00,\n",
      "          3.8913e-02,  2.6003e+00],\n",
      "        [-9.7786e+00, -1.4662e+01,  2.8510e+01,  ..., -5.1549e+00,\n",
      "          1.9186e+00,  4.5656e+00],\n",
      "        ...,\n",
      "        [ 4.1529e-01, -8.5492e-01,  4.1526e+00,  ..., -2.9795e+00,\n",
      "         -3.6206e-03,  2.5883e+00],\n",
      "        [ 4.8175e-01, -7.3641e-01,  3.9952e+00,  ..., -2.9380e+00,\n",
      "         -3.0475e-02,  2.5609e+00],\n",
      "        [-3.3304e-01, -2.2834e+00,  7.5668e+00,  ..., -3.9567e+00,\n",
      "          1.7550e-01,  3.5329e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 1.4960e+00, -4.7870e-01,  5.4361e+00,  ..., -4.6521e+00,\n",
      "          4.2082e-01,  3.7669e+00],\n",
      "        [ 1.3363e+00, -6.4897e-01,  5.6032e+00,  ..., -4.3903e+00,\n",
      "          4.3057e-01,  3.6621e+00],\n",
      "        [-7.2407e+00, -1.0879e+01,  2.1439e+01,  ..., -3.5556e+00,\n",
      "          3.0821e+00,  1.9159e+00],\n",
      "        ...,\n",
      "        [ 1.4200e+00, -7.5391e-02,  3.8552e+00,  ..., -3.7169e+00,\n",
      "          2.2445e-01,  3.0494e+00],\n",
      "        [-2.6309e+00, -5.8221e+00,  1.3595e+01,  ..., -4.0810e+00,\n",
      "          1.5999e+00,  2.8076e+00],\n",
      "        [ 1.4279e+00, -7.5537e-03,  3.7447e+00,  ..., -3.6134e+00,\n",
      "          1.5446e-01,  2.9967e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 0.4060, -2.6587,  9.7958,  ..., -5.3138,  1.8099,  3.6658],\n",
      "        [ 0.2000, -2.8013,  9.7887,  ..., -5.1113,  1.8345,  3.4845],\n",
      "        [ 1.1622, -0.6197,  4.7022,  ..., -3.5095,  0.8203,  2.5532],\n",
      "        ...,\n",
      "        [ 1.1354, -0.8698,  5.5838,  ..., -3.8783,  0.8963,  2.8611],\n",
      "        [ 0.1247, -3.5825, 13.0618,  ..., -6.5312,  2.3989,  4.2919],\n",
      "        [ 1.2182, -0.6434,  5.0037,  ..., -3.6580,  0.8319,  2.7000]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 1.2649, -0.1175,  3.6819,  ..., -2.7630,  0.7142,  2.0657],\n",
      "        [-0.1294, -3.6572, 11.9283,  ..., -5.7582,  2.9963,  3.2890],\n",
      "        [ 1.2529, -0.6797,  5.2551,  ..., -3.6818,  1.1826,  2.5541],\n",
      "        ...,\n",
      "        [ 2.0191, -1.5893,  9.8995,  ..., -6.9614,  2.3702,  4.4814],\n",
      "        [-0.1915, -3.6638, 11.7751,  ..., -5.6271,  2.9204,  3.2538],\n",
      "        [-0.8476, -4.9372, 16.1031,  ..., -7.6081,  4.0081,  4.0011]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 1.9457, -0.2639,  5.7520,  ..., -4.3649,  1.1083,  3.3289],\n",
      "        [ 1.6797, -0.7125,  6.4094,  ..., -4.5284,  1.2718,  3.3359],\n",
      "        [ 1.1842, -2.1946, 10.0884,  ..., -5.7905,  2.4534,  3.8145],\n",
      "        ...,\n",
      "        [ 1.9575, -2.2407, 12.0005,  ..., -7.5815,  2.6887,  5.1868],\n",
      "        [-0.1602, -4.6252, 14.8994,  ..., -7.1827,  3.4422,  4.5099],\n",
      "        [ 1.2909, -4.0252, 17.9209,  ..., -9.8779,  4.1130,  6.2826]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  2.2371,  -0.3746,   6.2350,  ...,  -4.6827,   1.1029,   3.6635],\n",
      "        [  1.9129,  -1.1984,   7.7503,  ...,  -5.2194,   1.5415,   3.9969],\n",
      "        [  2.0924,   0.7274,   2.4194,  ...,  -2.9296,   0.2831,   2.4828],\n",
      "        ...,\n",
      "        [  2.1111,   0.6415,   2.7096,  ...,  -3.0925,   0.3539,   2.5992],\n",
      "        [  1.9310,   0.5021,   2.6632,  ...,  -2.9453,   0.4054,   2.4322],\n",
      "        [ -2.6994, -11.2466,  33.8672,  ..., -13.1313,   7.1631,   8.3550]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 5.8708e+00, -1.5881e-03,  1.4140e+01,  ..., -1.0900e+01,\n",
      "          2.2753e+00,  9.0986e+00],\n",
      "        [ 1.9508e+00, -5.6644e-01,  6.8212e+00,  ..., -4.8046e+00,\n",
      "          1.1082e+00,  4.0170e+00],\n",
      "        [ 1.4625e+00, -2.8430e+00,  1.2503e+01,  ..., -7.3847e+00,\n",
      "          2.3828e+00,  5.6826e+00],\n",
      "        ...,\n",
      "        [ 4.9888e+00, -7.5498e-01,  1.3702e+01,  ..., -1.0628e+01,\n",
      "          2.5043e+00,  8.3304e+00],\n",
      "        [ 2.1101e+00, -1.9431e+00,  1.0906e+01,  ..., -7.1203e+00,\n",
      "          2.0099e+00,  5.5373e+00],\n",
      "        [ 3.1962e+00,  1.5079e-01,  6.9852e+00,  ..., -5.8070e+00,\n",
      "          1.1096e+00,  4.8446e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 3.6688e+00,  4.3480e-03,  8.8685e+00,  ..., -6.8930e+00,\n",
      "          1.5954e+00,  5.7386e+00],\n",
      "        [ 3.2757e+00, -1.0148e-01,  8.1777e+00,  ..., -6.4019e+00,\n",
      "          1.4683e+00,  5.2926e+00],\n",
      "        [ 3.2393e+00, -3.8475e-01,  8.9025e+00,  ..., -6.6961e+00,\n",
      "          1.6711e+00,  5.4740e+00],\n",
      "        ...,\n",
      "        [ 3.5564e+00, -1.9523e-01,  9.0909e+00,  ..., -6.9817e+00,\n",
      "          1.6702e+00,  5.7512e+00],\n",
      "        [ 3.5349e-01, -5.0013e+00,  1.7077e+01,  ..., -8.5934e+00,\n",
      "          3.4096e+00,  6.6600e+00],\n",
      "        [ 3.1002e+00, -5.5062e-01,  8.5454e+00,  ..., -6.6099e+00,\n",
      "          1.8008e+00,  5.2127e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 3.3436, -0.2750,  8.4635,  ..., -6.4877,  1.5503,  5.4714],\n",
      "        [ 3.8756,  0.4348,  7.3382,  ..., -6.3491,  1.4189,  5.2956],\n",
      "        [ 3.5934,  0.1397,  8.0426,  ..., -6.4742,  1.4316,  5.5002],\n",
      "        ...,\n",
      "        [ 3.7518,  0.0210,  8.8715,  ..., -6.9502,  1.6294,  5.8685],\n",
      "        [ 3.3993, -0.4482,  8.9335,  ..., -6.9750,  1.9547,  5.5535],\n",
      "        [ 3.3089, -0.2835,  8.4460,  ..., -6.4490,  1.5469,  5.4460]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  3.8250,   0.6884,   5.6116,  ...,  -5.4656,   1.1096,   4.6196],\n",
      "        [ -2.6786, -10.6167,  30.9704,  ..., -13.0465,   5.6495,  11.3691],\n",
      "        [ -2.5624, -11.3229,  33.8566,  ..., -14.3956,   6.2555,  12.5255],\n",
      "        ...,\n",
      "        [  4.2522,   1.9834,   2.8081,  ...,  -4.3863,   0.4428,   3.7275],\n",
      "        [  4.7640,   1.7176,   4.3466,  ...,  -5.5453,   0.9557,   4.6185],\n",
      "        [  5.1734,   1.6713,   5.0761,  ...,  -6.4494,   1.4091,   5.0966]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-1.0891, -5.3489, 15.5420,  ..., -6.9998,  2.8895,  6.1276],\n",
      "        [ 5.4446,  2.7009,  3.5031,  ..., -5.3687,  0.8478,  4.3928],\n",
      "        [ 3.6314,  0.7513,  5.5694,  ..., -5.1861,  1.3586,  4.2592],\n",
      "        ...,\n",
      "        [ 4.8407,  2.3418,  3.2481,  ..., -4.8357,  0.7469,  3.9764],\n",
      "        [ 0.1254, -4.7005, 16.0568,  ..., -8.0311,  3.3033,  6.8215],\n",
      "        [ 3.4015,  0.3237,  6.6050,  ..., -5.5140,  1.5718,  4.5357]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  1.0778,  -2.0345,   9.3134,  ...,  -5.2407,   2.1016,   4.3718],\n",
      "        [  5.1841,   2.0562,   4.3729,  ...,  -5.7610,   1.7328,   4.2020],\n",
      "        [  2.6182,  -0.3788,   6.7099,  ...,  -5.0483,   1.8320,   3.9300],\n",
      "        ...,\n",
      "        [  5.5218,   2.7488,   3.1151,  ...,  -5.1725,   1.2023,   3.8264],\n",
      "        [ -3.9504, -14.1707,  41.2418,  ..., -17.8860,   9.6800,  14.8623],\n",
      "        [  2.0325,   0.0978,   3.8876,  ...,  -3.2994,   1.0488,   2.5796]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ -0.8598,  -4.9649,  14.9045,  ...,  -6.8433,   4.1330,   5.0616],\n",
      "        [  1.2371,  -2.1829,  10.5359,  ...,  -5.9585,   3.2884,   4.1663],\n",
      "        [ -5.7946, -10.8862,  25.0701,  ...,  -8.5072,   5.5348,   7.4269],\n",
      "        ...,\n",
      "        [  4.8552,   2.0336,   3.8961,  ...,  -4.9837,   2.0223,   3.1223],\n",
      "        [  4.7046,   1.9409,   3.8910,  ...,  -4.8483,   1.9826,   3.0516],\n",
      "        [  4.8230,   2.0060,   3.8936,  ...,  -4.9154,   1.9664,   3.1099]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  4.3847,   1.9915,   2.7884,  ...,  -4.0298,   1.7127,   2.3658],\n",
      "        [  3.5773,   1.4705,   2.5278,  ...,  -3.4559,   1.4706,   2.0615],\n",
      "        [  4.2145,   1.8384,   2.8768,  ...,  -3.9902,   1.7162,   2.3444],\n",
      "        ...,\n",
      "        [  3.1105,   0.9099,   3.4239,  ...,  -3.5640,   1.6468,   2.1868],\n",
      "        [  1.5343,  -1.5388,   8.7310,  ...,  -5.0473,   2.7370,   3.6092],\n",
      "        [-11.1572, -19.9222,  46.2542,  ..., -14.0235,   8.9446,  13.6975]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.6667,  2.0304,  3.2765,  ..., -4.3450,  1.8011,  2.7083],\n",
      "        [ 5.2712,  2.3377,  3.6874,  ..., -4.8971,  2.0240,  3.0428],\n",
      "        [ 1.0076, -1.2646,  6.6427,  ..., -3.5314,  1.5771,  2.9562],\n",
      "        ...,\n",
      "        [ 4.7917,  2.0815,  3.4187,  ..., -4.4967,  1.8460,  2.8072],\n",
      "        [ 0.7714, -1.6145,  7.3528,  ..., -3.6872,  1.6845,  3.1472],\n",
      "        [ 0.1771, -2.2048,  8.0125,  ..., -3.6418,  1.7949,  3.1424]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  3.7808,   0.8331,   5.5220,  ...,  -4.7107,   1.8075,   3.5025],\n",
      "        [  4.8919,   1.2945,   6.6242,  ...,  -5.8074,   2.1952,   4.3144],\n",
      "        [  4.1294,   0.9505,   5.9862,  ...,  -5.1099,   1.9205,   3.8259],\n",
      "        ...,\n",
      "        [  4.0166,   1.1620,   4.8184,  ...,  -4.5496,   1.6948,   3.2806],\n",
      "        [  5.0673,   1.5293,   6.2369,  ...,  -5.7420,   2.2010,   4.1876],\n",
      "        [ -8.2490, -14.4408,  33.5039,  ...,  -9.3053,   3.8128,  11.7710]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-2.7677, -7.6551, 21.5196,  ..., -6.9854,  2.2117,  8.5934],\n",
      "        [ 0.6709, -4.1953, 16.9675,  ..., -7.2043,  2.3969,  7.7067],\n",
      "        [ 3.1237, -1.1980, 11.9413,  ..., -6.6974,  2.1908,  6.2451],\n",
      "        ...,\n",
      "        [ 3.2293,  0.9450,  4.1776,  ..., -3.6112,  1.0495,  2.8687],\n",
      "        [ 4.1603,  1.1767,  5.0679,  ..., -4.6371,  1.4197,  3.6158],\n",
      "        [ 2.5878,  0.7128,  3.2650,  ..., -2.8502,  0.8546,  2.2634]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 5.0403,  2.4838,  3.0381,  ..., -4.0605,  1.1158,  2.9211],\n",
      "        [ 1.7536, -1.0476,  8.0462,  ..., -4.0594,  1.3545,  3.8310],\n",
      "        [ 3.8816,  0.6189,  6.8153,  ..., -4.8884,  1.5256,  4.0840],\n",
      "        ...,\n",
      "        [ 3.9363,  0.6379,  6.8686,  ..., -4.9360,  1.5354,  4.1252],\n",
      "        [ 2.6781, -0.8516,  9.3534,  ..., -5.1063,  1.8573,  4.5684],\n",
      "        [ 1.6394,  0.0763,  3.3553,  ..., -2.2335,  0.6740,  1.9274]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  4.1902,   1.3429,   5.1491,  ...,  -4.2468,   1.4338,   3.1700],\n",
      "        [  1.8421,  -2.0714,  12.6524,  ...,  -5.4810,   2.5450,   4.7137],\n",
      "        [ -4.2955, -11.4191,  33.0394,  ...,  -8.9647,   5.3223,   9.1280],\n",
      "        ...,\n",
      "        [  2.4546,   0.7222,   3.1273,  ...,  -2.4887,   0.8402,   1.8534],\n",
      "        [  1.9577,   0.5415,   2.4964,  ...,  -2.0074,   0.6693,   1.5372],\n",
      "        [  4.3053,   1.8180,   3.8969,  ...,  -3.8210,   1.2198,   2.7869]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.5293,  0.6881,  9.1673,  ..., -5.5519,  2.5550,  3.8623],\n",
      "        [ 4.4118,  1.3972,  5.8286,  ..., -4.4636,  1.8456,  2.9850],\n",
      "        [ 5.1981,  1.7624,  6.5286,  ..., -5.1335,  2.1310,  3.4145],\n",
      "        ...,\n",
      "        [ 4.4876,  1.4286,  5.9135,  ..., -4.5286,  1.8714,  3.0284],\n",
      "        [ 4.4506,  1.4741,  5.6286,  ..., -4.4148,  1.7827,  2.9370],\n",
      "        [ 4.7020,  1.5259,  6.1151,  ..., -4.7111,  1.9437,  3.1634]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  4.9631,   2.7194,   2.2763,  ...,  -3.6772,   0.9516,   2.4064],\n",
      "        [  4.9905,   2.7362,   2.2866,  ...,  -3.6955,   0.9589,   2.4178],\n",
      "        [  5.0549,   2.7830,   2.3211,  ...,  -3.7445,   0.9690,   2.4384],\n",
      "        ...,\n",
      "        [  3.2513,  -0.5998,  11.2448,  ...,  -5.5029,   2.7684,   3.9560],\n",
      "        [ -4.0980, -11.8864,  36.9021,  ...,  -9.2705,   7.2405,   7.6165],\n",
      "        [  7.0372,   4.2501,   1.9102,  ...,  -4.7350,   1.1425,   3.0572]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.2890,  2.5251,  1.3148,  ..., -3.0114,  0.4912,  2.1018],\n",
      "        [ 3.2148, -0.0110,  8.8219,  ..., -4.9993,  2.2467,  3.4580],\n",
      "        [ 5.4972,  3.3047,  1.6196,  ..., -3.8332,  0.6708,  2.6678],\n",
      "        ...,\n",
      "        [ 2.7674,  1.2849,  2.2461,  ..., -2.4034,  0.6211,  1.6920],\n",
      "        [ 5.7361,  3.4563,  1.6940,  ..., -3.9914,  0.7041,  2.7804],\n",
      "        [ 4.3914,  2.5649,  1.4066,  ..., -3.1012,  0.5403,  2.1594]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 5.7745,  3.5907,  1.5778,  ..., -4.1050,  0.3854,  3.1488],\n",
      "        [ 6.2854,  3.9665,  1.3881,  ..., -4.3712,  0.3244,  3.3336],\n",
      "        [-0.1838, -3.1092, 13.0208,  ..., -5.2365,  3.2419,  3.4301],\n",
      "        ...,\n",
      "        [ 4.8298,  2.9751,  1.1526,  ..., -3.4164,  0.2429,  2.5947],\n",
      "        [ 3.2197,  1.9320,  0.8392,  ..., -2.2766,  0.1618,  1.7147],\n",
      "        [ 4.9592,  3.0752,  1.1412,  ..., -3.4758,  0.2247,  2.6522]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 3.4681,  2.1151,  1.2286,  ..., -2.7844,  0.2880,  1.9995],\n",
      "        [ 5.3795,  3.4602,  1.0348,  ..., -3.8516,  0.2209,  2.8961],\n",
      "        [ 2.0415,  0.2649,  6.1446,  ..., -4.5027,  1.9132,  2.7457],\n",
      "        ...,\n",
      "        [ 5.0472,  3.2476,  0.9370,  ..., -3.6004,  0.1711,  2.7093],\n",
      "        [ 6.0001,  3.8916,  1.1048,  ..., -4.2716,  0.2566,  3.1967],\n",
      "        [ 6.1017,  3.9621,  1.1229,  ..., -4.3391,  0.2401,  3.2709]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.9393,  3.2292,  1.3172,  ..., -4.0561,  0.4833,  2.7775],\n",
      "        [ 5.1447,  3.3658,  1.3994,  ..., -4.2200,  0.5094,  2.9013],\n",
      "        [ 5.2551,  3.4370,  1.4327,  ..., -4.2988,  0.5184,  2.9644],\n",
      "        ...,\n",
      "        [ 4.2448,  2.7323,  1.2635,  ..., -3.5776,  0.4568,  2.4387],\n",
      "        [-8.5106, -7.2426, 14.3800,  ..., -5.8265,  3.9340,  2.7450],\n",
      "        [ 1.1594,  0.5311,  1.4639,  ..., -1.7647,  0.4381,  1.1147]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  1.9771,   1.3281,   1.7484,  ...,  -3.0901,   0.4310,   2.0815],\n",
      "        [  4.4026,   2.9393,   0.8635,  ...,  -3.5741,   0.4090,   2.3667],\n",
      "        [  4.6053,   3.0962,   0.9039,  ...,  -3.7521,   0.4378,   2.4658],\n",
      "        ...,\n",
      "        [  4.6265,   3.1075,   0.9057,  ...,  -3.7646,   0.4464,   2.4720],\n",
      "        [-10.2136,  -6.5217,   9.4366,  ...,  -5.0552,   0.7276,   3.9621],\n",
      "        [  4.6803,   3.1387,   0.9173,  ...,  -3.7946,   0.4461,   2.5036]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-2.8270e-01,  1.7162e-01,  1.9460e+00,  ..., -3.1806e+00,\n",
      "         -1.5128e-01,  2.5977e+00],\n",
      "        [-1.3144e+00, -2.9924e-01,  2.3255e+00,  ..., -3.6585e+00,\n",
      "         -4.3811e-01,  3.1441e+00],\n",
      "        [ 4.5652e+00,  3.2789e+00,  1.2928e+00,  ..., -4.7312e+00,\n",
      "          4.6829e-01,  3.1277e+00],\n",
      "        ...,\n",
      "        [ 6.1989e+00,  4.2079e+00,  1.1131e+00,  ..., -5.0628e+00,\n",
      "          7.1088e-01,  3.1308e+00],\n",
      "        [ 7.5221e-01,  7.5039e-01,  1.3206e+00,  ..., -2.8039e+00,\n",
      "          1.0992e-04,  2.1337e+00],\n",
      "        [ 4.2582e+00,  3.0380e+00,  1.2040e+00,  ..., -4.4153e+00,\n",
      "          4.2968e-01,  2.9176e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 5.1985,  3.4446,  1.1301,  ..., -4.2373,  0.8669,  2.3593],\n",
      "        [-0.3024,  0.5983,  1.0850,  ..., -3.3579, -0.4792,  2.8592],\n",
      "        [ 4.9475,  3.2829,  1.0471,  ..., -4.1146,  0.8073,  2.3036],\n",
      "        ...,\n",
      "        [ 5.1716,  3.4279,  1.1178,  ..., -4.2295,  0.8641,  2.3569],\n",
      "        [ 3.0592,  2.2816,  1.0925,  ..., -3.6601,  0.4190,  2.3310],\n",
      "        [ 5.5463,  3.7082,  1.1669,  ..., -4.5579,  0.8957,  2.5550]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.4062,  3.0052,  1.3841,  ..., -4.3337,  0.9817,  2.3130],\n",
      "        [-2.3123, -0.3928,  1.6492,  ..., -3.5831, -0.9513,  3.4502],\n",
      "        [ 4.3770,  2.9751,  1.3699,  ..., -4.2942,  0.9713,  2.3049],\n",
      "        ...,\n",
      "        [ 4.1216,  2.7425,  1.3116,  ..., -4.0060,  0.9376,  2.1382],\n",
      "        [ 4.4873,  3.0628,  1.4208,  ..., -4.4026,  0.9917,  2.3592],\n",
      "        [ 1.8329,  1.4441,  1.1614,  ..., -3.1281,  0.3369,  1.9877]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  2.9187,   2.0812,   1.4256,  ...,  -3.6907,   0.7767,   2.0326],\n",
      "        [  4.5373,   2.9216,   1.6248,  ...,  -4.2806,   1.2582,   2.0447],\n",
      "        [  3.2089,   2.2151,   1.4928,  ...,  -3.7836,   0.8617,   2.0304],\n",
      "        ...,\n",
      "        [-16.8221,  -7.2573,   2.2831,  ...,  -2.8166,  -5.4669,   6.9037],\n",
      "        [  2.9842,   2.0408,   1.4056,  ...,  -3.5582,   0.8227,   1.9110],\n",
      "        [  3.0364,   2.1625,   1.4777,  ...,  -3.7988,   0.8038,   2.0781]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  5.3868,   2.9919,   2.6470,  ...,  -4.8512,   1.7996,   2.3381],\n",
      "        [-10.7693,  -4.4568,   1.8644,  ...,  -2.3813,  -3.5013,   4.8215],\n",
      "        [  5.2966,   2.9183,   2.6209,  ...,  -4.7372,   1.7774,   2.2745],\n",
      "        ...,\n",
      "        [  5.0785,   2.8805,   2.6024,  ...,  -4.7586,   1.7241,   2.3338],\n",
      "        [  5.3078,   2.9360,   2.6244,  ...,  -4.7887,   1.7782,   2.3018],\n",
      "        [ -4.3582,  -1.2646,   1.8866,  ...,  -3.5676,  -1.5091,   4.0818]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[-2.7671, -1.4867,  4.2687,  ..., -2.4731, -0.8751,  3.3732],\n",
      "        [-0.1725, -0.2291,  3.5001,  ..., -2.7061,  0.0831,  2.5598],\n",
      "        [ 0.8798,  0.2878,  3.4020,  ..., -2.9640,  0.4289,  2.4766],\n",
      "        ...,\n",
      "        [ 4.5233,  1.9835,  3.2616,  ..., -4.0733,  1.7133,  2.1892],\n",
      "        [ 1.6384,  0.6591,  3.3318,  ..., -3.1755,  0.7125,  2.3700],\n",
      "        [ 4.9134,  2.2432,  3.4374,  ..., -4.4109,  1.8285,  2.3587]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  1.9055,   0.3350,   4.1036,  ...,  -2.7047,   0.6765,   2.3541],\n",
      "        [  2.1099,   0.4230,   3.8852,  ...,  -2.6906,   0.7880,   2.2206],\n",
      "        [  2.1613,   0.4400,   3.9576,  ...,  -2.7410,   0.8046,   2.2727],\n",
      "        ...,\n",
      "        [  2.3567,   0.5280,   3.8978,  ...,  -2.7903,   0.8559,   2.2508],\n",
      "        [-11.0207,  -6.8179,  11.1063,  ...,   0.2855,  -3.6778,   4.9693],\n",
      "        [  2.9866,   0.8529,   3.9084,  ...,  -3.1029,   1.0838,   2.2965]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 3.4695,  0.6103,  4.7801,  ..., -3.3788,  1.2532,  2.5828],\n",
      "        [ 1.0343,  0.1463,  3.7733,  ..., -0.7666, -0.0333,  2.2782],\n",
      "        [ 1.4170,  0.2401,  4.1068,  ..., -1.1790,  0.1083,  2.4725],\n",
      "        ...,\n",
      "        [ 3.5534,  0.6355,  4.8953,  ..., -3.4752,  1.2687,  2.6473],\n",
      "        [ 3.5506,  0.6418,  4.9048,  ..., -3.4765,  1.2614,  2.6493],\n",
      "        [ 3.4885,  0.6230,  4.8091,  ..., -3.4069,  1.2521,  2.5936]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  2.1377,   0.1508,   3.7245,  ...,  -1.7167,   0.5710,   1.9886],\n",
      "        [  2.2413,  -0.2537,   4.9052,  ...,  -2.7291,   0.8985,   2.3221],\n",
      "        [  7.5589,   9.7870, -10.1904,  ...,  14.9473,  -4.4122,   1.7769],\n",
      "        ...,\n",
      "        [  2.3944,  -0.2404,   5.2673,  ...,  -3.0088,   0.9026,   2.5422],\n",
      "        [  2.3499,  -0.2388,   5.0694,  ...,  -2.8526,   0.8874,   2.4495],\n",
      "        [  3.2403,   1.9633,   1.3552,  ...,   1.3396,  -0.3296,   2.0983]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.1715, -0.3058, -3.3100,  ..., 17.4161, -3.4548,  0.8523],\n",
      "        [ 2.5944,  0.2516,  4.2144,  ..., -2.3334,  0.5922,  2.4380],\n",
      "        [ 2.6843,  0.2962,  4.3683,  ..., -2.4601,  0.5918,  2.5471],\n",
      "        ...,\n",
      "        [ 2.6856,  0.3032,  4.4029,  ..., -2.4926,  0.5874,  2.5587],\n",
      "        [ 2.1949,  0.1308,  2.5989,  ...,  0.1147,  0.1008,  1.7066],\n",
      "        [ 2.7018,  0.3036,  4.4304,  ..., -2.5228,  0.5926,  2.5782]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 3.3087,  0.7673,  4.4785,  ..., -2.1983,  0.5986,  2.6486],\n",
      "        [ 2.0599, -0.8180,  3.2268,  ..., -0.2430,  0.1090,  1.9610],\n",
      "        [ 2.1004, -0.6649,  3.1961,  ..., -0.4026,  0.1537,  1.9332],\n",
      "        ...,\n",
      "        [ 3.3314,  0.7930,  4.5267,  ..., -2.2337,  0.5866,  2.6776],\n",
      "        [ 3.3370,  0.8470,  4.5597,  ..., -2.2883,  0.5742,  2.6952],\n",
      "        [ 3.3328,  0.8396,  4.5593,  ..., -2.2823,  0.5833,  2.6918]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 3.9054,  1.3020,  4.5230,  ..., -1.3642,  0.5267,  2.4352],\n",
      "        [ 3.9488,  1.2816,  4.5350,  ..., -1.3762,  0.5561,  2.4322],\n",
      "        [ 3.9747,  1.2940,  4.5570,  ..., -1.3851,  0.5779,  2.4578],\n",
      "        ...,\n",
      "        [ 4.1144,  1.4814,  4.6742,  ..., -1.4095,  0.5453,  2.5346],\n",
      "        [ 4.0822,  1.4026,  4.6241,  ..., -1.4003,  0.5501,  2.5022],\n",
      "        [ 4.0164,  1.4082,  4.6096,  ..., -1.3917,  0.5455,  2.4811]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.4297e+00,  1.6055e+00,  4.5139e+00,  ..., -7.3622e-01,\n",
      "          4.3592e-01,  2.3907e+00],\n",
      "        [-7.6916e-03, -8.5800e+00,  8.0795e+00,  ..., -6.3734e+00,\n",
      "          4.8500e-01,  5.2601e+00],\n",
      "        [ 4.3413e+00,  1.5379e+00,  4.4483e+00,  ..., -7.4746e-01,\n",
      "          4.4696e-01,  2.3544e+00],\n",
      "        ...,\n",
      "        [ 4.4996e+00,  1.6030e+00,  4.6127e+00,  ..., -7.8270e-01,\n",
      "          4.3380e-01,  2.4727e+00],\n",
      "        [ 3.1301e+00, -1.0037e+00,  5.2700e+00,  ..., -2.0768e+00,\n",
      "          5.4174e-01,  2.9177e+00],\n",
      "        [ 1.7620e-01, -8.3203e+00,  8.1262e+00,  ..., -6.2706e+00,\n",
      "          4.5958e-01,  5.2992e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 2.8984e+00, -3.1343e-01,  4.2883e+00,  ..., -1.4603e+00,\n",
      "          4.0587e-01,  2.2303e+00],\n",
      "        [-1.1222e+00, -1.1153e+01,  1.0141e+01,  ..., -9.9557e+00,\n",
      "          1.0511e+00,  6.2011e+00],\n",
      "        [-7.9981e+00, -2.8626e+01,  1.9735e+01,  ..., -2.3483e+01,\n",
      "          2.5613e+00,  1.1991e+01],\n",
      "        ...,\n",
      "        [ 4.2407e+00,  1.8659e+00,  3.7038e+00,  ..., -6.5938e-03,\n",
      "          2.7393e-01,  1.8334e+00],\n",
      "        [ 4.2189e+00,  1.8474e+00,  3.6875e+00,  ..., -6.8997e-03,\n",
      "          2.9914e-01,  1.8244e+00],\n",
      "        [ 4.1863e+00,  1.8859e+00,  3.7116e+00,  ...,  8.5887e-04,\n",
      "          2.7612e-01,  1.8264e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 3.7401e+00,  1.7920e+00,  2.9272e+00,  ...,  2.5476e-01,\n",
      "          5.6886e-02,  1.5335e+00],\n",
      "        [ 3.6293e+00,  1.6808e+00,  2.9751e+00,  ...,  1.9199e-01,\n",
      "          6.4768e-02,  1.5129e+00],\n",
      "        [ 3.8508e+00,  1.9973e+00,  2.9486e+00,  ...,  3.8560e-01,\n",
      "          1.4641e-02,  1.5192e+00],\n",
      "        ...,\n",
      "        [-1.3137e+01, -4.1195e+01,  2.9415e+01,  ..., -3.7376e+01,\n",
      "          5.5330e+00,  1.5553e+01],\n",
      "        [ 3.9088e+00,  1.9604e+00,  3.0049e+00,  ...,  3.5001e-01,\n",
      "          3.3379e-02,  1.5525e+00],\n",
      "        [ 3.9069e+00,  2.0567e+00,  2.9653e+00,  ...,  4.2154e-01,\n",
      "          5.7732e-03,  1.5333e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  4.3212,   2.0973,   3.2103,  ...,   0.1116,   0.0211,   1.7651],\n",
      "        [  4.5897,   2.3673,   3.2864,  ...,   0.2691,  -0.0633,   1.8270],\n",
      "        [  4.5638,   2.3420,   3.2999,  ...,   0.2456,  -0.0520,   1.8269],\n",
      "        ...,\n",
      "        [ -3.3384, -13.8644,  12.2531,  ..., -14.0112,   2.3462,   5.9972],\n",
      "        [  3.6421,   1.2314,   3.4292,  ...,  -0.4896,   0.1719,   1.7382],\n",
      "        [  1.4949,  -4.0821,   6.9934,  ...,  -5.4049,   0.9114,   3.5020]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.6025,  2.5708,  2.9639,  ...,  0.0597, -0.1923,  1.8690],\n",
      "        [ 3.3570,  0.8067,  3.6024,  ..., -1.2422,  0.0842,  1.9947],\n",
      "        [ 4.5948,  2.5772,  2.9457,  ...,  0.0683, -0.2104,  1.8631],\n",
      "        ...,\n",
      "        [ 4.7191,  2.7078,  2.9734,  ...,  0.1359, -0.2454,  1.8924],\n",
      "        [ 1.6719, -3.4439,  6.6772,  ..., -5.2529,  0.8706,  3.2649],\n",
      "        [ 4.7084,  2.6763,  2.9794,  ...,  0.1222, -0.2382,  1.8923]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.3619,  2.4189,  2.8148,  ..., -0.4500, -0.2715,  1.9420],\n",
      "        [ 4.3885,  2.4053,  2.8353,  ..., -0.4573, -0.2693,  1.9476],\n",
      "        [ 4.3832,  2.4464,  2.8087,  ..., -0.4313, -0.2801,  1.9438],\n",
      "        ...,\n",
      "        [ 4.2367,  2.2680,  2.7954,  ..., -0.5195, -0.2272,  1.9168],\n",
      "        [ 4.4182,  2.3993,  2.9448,  ..., -0.5295, -0.2467,  1.9635],\n",
      "        [ 4.3091,  2.3516,  2.8274,  ..., -0.4936, -0.2532,  1.9367]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 0.2240, -6.5449,  9.2412,  ..., -9.2363,  1.6240,  4.1824],\n",
      "        [ 5.2436,  3.1494,  3.0114,  ..., -0.8873, -0.3413,  2.2880],\n",
      "        [ 5.3544,  3.3196,  3.0006,  ..., -0.8148, -0.3897,  2.3134],\n",
      "        ...,\n",
      "        [ 5.3428,  3.2862,  3.0207,  ..., -0.8359, -0.3738,  2.3270],\n",
      "        [ 0.6189, -5.3785,  8.1887,  ..., -8.0076,  1.3988,  3.7466],\n",
      "        [ 4.9621,  2.9106,  2.9376,  ..., -0.9478, -0.2691,  2.1989]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 1.3447, -3.7724,  7.2785,  ..., -7.0787,  1.1654,  3.4352],\n",
      "        [ 5.1957,  3.6063,  2.4256,  ..., -0.8319, -0.4085,  2.0936],\n",
      "        [ 5.7437,  4.1384,  2.5932,  ..., -0.7978, -0.5537,  2.3178],\n",
      "        ...,\n",
      "        [ 5.4819,  3.9541,  2.4426,  ..., -0.7101, -0.4893,  2.1722],\n",
      "        [ 4.9641,  3.3102,  2.4632,  ..., -0.9779, -0.3322,  2.0603],\n",
      "        [ 5.0001,  3.4255,  2.4270,  ..., -0.8968, -0.3539,  2.0355]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  4.2591,   2.8818,   2.1359,  ...,  -1.2753,  -0.1826,   1.8197],\n",
      "        [  4.4653,   3.1866,   2.0991,  ...,  -1.1365,  -0.2769,   1.8558],\n",
      "        [  4.2063,   2.8097,   2.1888,  ...,  -1.3378,  -0.1996,   1.8520],\n",
      "        ...,\n",
      "        [  4.1490,   2.6949,   2.2034,  ...,  -1.4061,  -0.1547,   1.8658],\n",
      "        [  4.2552,   2.8393,   2.1775,  ...,  -1.3295,  -0.2035,   1.8655],\n",
      "        [-13.9767, -42.4868,  40.6512,  ..., -47.9430,  10.4072,  16.1975]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  6.3604,   4.3704,   3.2545,  ...,  -2.7260,  -0.1345,   2.8939],\n",
      "        [ -0.3010,  -9.6549,  13.4812,  ..., -16.0620,   2.6722,   7.1349],\n",
      "        [  6.5225,   4.5495,   3.2509,  ...,  -2.6882,  -0.1791,   2.9440],\n",
      "        ...,\n",
      "        [  6.1192,   4.1719,   3.1208,  ...,  -2.6492,  -0.1188,   2.7897],\n",
      "        [  6.3262,   4.3667,   3.1823,  ...,  -2.6503,  -0.1475,   2.8506],\n",
      "        [ -1.5018, -13.1131,  16.6291,  ..., -20.0933,   3.3687,   8.6764]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 4.5208,  3.2431,  2.1055,  ..., -2.1088,  0.0082,  1.9218],\n",
      "        [ 4.3172,  2.9748,  2.1400,  ..., -2.2112,  0.0527,  1.9365],\n",
      "        [ 3.6331,  1.5698,  3.4077,  ..., -3.8060,  0.3027,  2.5335],\n",
      "        ...,\n",
      "        [ 4.7570,  3.6527,  1.9927,  ..., -1.8708, -0.0794,  1.9013],\n",
      "        [ 4.4065,  3.0930,  2.1270,  ..., -2.1638,  0.0143,  1.9437],\n",
      "        [ 4.9776,  3.9042,  1.9546,  ..., -1.8077, -0.1342,  1.9516]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  3.7310,  -1.1478,   7.3379,  ...,  -9.4101,   1.3955,   4.8660],\n",
      "        [  6.7136,   4.9417,   3.2265,  ...,  -3.4807,   0.1826,   2.8361],\n",
      "        [  0.2871,  -9.1221,  13.4242,  ..., -18.1061,   2.7739,   8.2762],\n",
      "        ...,\n",
      "        [  5.8973,   4.0821,   3.1011,  ...,  -3.4914,   0.2922,   2.6339],\n",
      "        [  6.4935,   4.7057,   3.2121,  ...,  -3.4903,   0.2100,   2.7686],\n",
      "        [  1.9893,  -6.0753,  11.7868,  ..., -15.5801,   2.3298,   7.4349]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  5.4627,   3.9519,   2.7664,  ...,  -3.0950,   0.4139,   2.1377],\n",
      "        [ -5.9058, -29.5958,  33.0192,  ..., -47.3077,   7.6133,  20.2971],\n",
      "        [  5.3534,   3.7797,   2.9890,  ...,  -3.3524,   0.4955,   2.2147],\n",
      "        ...,\n",
      "        [  5.7805,   4.3082,   2.9584,  ...,  -3.2196,   0.4193,   2.2537],\n",
      "        [ -2.5938, -19.0242,  23.2021,  ..., -32.9905,   5.2135,  14.4552],\n",
      "        [  5.4954,   3.9845,   2.7803,  ...,  -3.1316,   0.3872,   2.1737]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  4.9874,   3.2669,   3.1992,  ...,  -3.9036,   0.6298,   2.2800],\n",
      "        [  1.7543,  -5.1480,  10.1528,  ..., -14.4152,   2.4147,   6.5249],\n",
      "        [ -1.7467, -17.0937,  21.3406,  ..., -31.5552,   5.0956,  13.7122],\n",
      "        ...,\n",
      "        [ -0.7936, -15.2366,  20.2874,  ..., -29.8342,   4.7365,  13.1603],\n",
      "        [  5.5382,   3.8885,   3.2245,  ...,  -3.8194,   0.5552,   2.3542],\n",
      "        [  5.5236,   3.8070,   3.3022,  ...,  -3.9613,   0.5708,   2.4265]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[  5.4694,   4.0254,   3.0195,  ...,  -3.6063,   0.5144,   2.1698],\n",
      "        [  5.5962,   4.1576,   3.0763,  ...,  -3.6497,   0.4981,   2.2164],\n",
      "        [  5.5718,   4.1664,   3.0716,  ...,  -3.6115,   0.5140,   2.1701],\n",
      "        ...,\n",
      "        [  5.5684,   4.1173,   3.1018,  ...,  -3.6862,   0.5161,   2.2221],\n",
      "        [ -5.2854, -31.2949,  35.9722,  ..., -54.8376,   9.0984,  23.2974],\n",
      "        [  5.2642,   3.8463,   2.9674,  ...,  -3.5529,   0.5187,   2.1046]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current run is terminating due to exception: confidences should sum to 1\n",
      "Engine run is terminating due to exception: confidences should sum to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[ 5.0182,  4.3311,  2.1572,  ..., -2.2596,  0.0653,  1.6861],\n",
      "        [ 4.5607,  3.7568,  2.2179,  ..., -2.4188,  0.1742,  1.6295],\n",
      "        [ 4.0324,  3.0440,  2.3511,  ..., -2.7465,  0.3143,  1.6398],\n",
      "        ...,\n",
      "        [ 4.9635,  4.2752,  2.1515,  ..., -2.2598,  0.0722,  1.6733],\n",
      "        [ 4.0914,  3.1429,  2.2961,  ..., -2.6573,  0.2881,  1.6290],\n",
      "        [ 4.3031,  3.4013,  2.2421,  ..., -2.5457,  0.2012,  1.6421]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n",
      "torch.Size([12, 25, 224, 224])\n",
      "x:tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "x.shape:torch.Size([12, 303])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "confidences should sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m manager\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     60\u001b[0m manager\u001b[38;5;241m.\u001b[39miters_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/engine/engine.py:892\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_resume_enabled:\n\u001b[0;32m--> 892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/engine/engine.py:935\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_as_gen()\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/engine/engine.py:993\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/engine/engine.py:959\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[0;32m--> 959\u001b[0m epoch_time_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[1;32m    961\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m epoch_time_taken\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/engine/engine.py:1087\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1087\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/engine/engine.py:638\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/ignite/engine/engine.py:1068\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_STARTED)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[0;32m-> 1068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED)\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mcreate_trainer.<locals>.update_fn\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43melem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mLyftMultiRegressor.forward\u001b[0;34m(self, image, targets, target_availabilities)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, targets, target_availabilities):\n\u001b[1;32m     10\u001b[0m     pred, confidences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(image)\n\u001b[0;32m---> 11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlossfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_availabilities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnll\u001b[39m\u001b[38;5;124m\"\u001b[39m: pytorch_neg_multi_log_likelihood_batch(targets, pred, confidences, target_availabilities)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m     ppe\u001b[38;5;241m.\u001b[39mreporting\u001b[38;5;241m.\u001b[39mreport(metrics, \u001b[38;5;28mself\u001b[39m)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mpytorch_neg_multi_log_likelihood_batch\u001b[0;34m(gt, pred, confidences, avails)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m gt\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (batch_size, future_len, num_coords), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 2D (Time x Coords) array for gt, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m confidences\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (batch_size, num_modes), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 1D (Modes) array for gt, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidences\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(torch\u001b[38;5;241m.\u001b[39msum(confidences, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), confidences\u001b[38;5;241m.\u001b[39mnew_ones((batch_size,))), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidences should sum to 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m avails\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (batch_size, future_len), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 1D (Time) array for gt, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavails\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# assert all data are valid\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: confidences should sum to 1"
     ]
    }
   ],
   "source": [
    "# Train setup\n",
    "trainer = create_trainer(model, optimizer, device)\n",
    "\n",
    "\n",
    "def eval_func(*batch):\n",
    "    loss, metrics = model(*[elem.to(device) for elem in batch])\n",
    "\n",
    "\n",
    "valid_evaluator = E.Evaluator(\n",
    "    valid_loader,\n",
    "    model,\n",
    "    progress_bar=False,\n",
    "    eval_func=eval_func,\n",
    ")\n",
    "\n",
    "log_trigger = (10 if debug else 1000, \"iteration\")\n",
    "log_report = E.LogReport(trigger=log_trigger)\n",
    "\n",
    "\n",
    "extensions = [\n",
    "    log_report,  # Save `log` to file\n",
    "    valid_evaluator,  # Run evaluation for valid dataset in each epoch.\n",
    "    # E.FailOnNonNumber()  # Stop training when nan is detected.\n",
    "    E.ProgressBarNotebook(update_interval=10 if debug else 100),  # Show progress bar during training\n",
    "    E.PrintReportNotebook(),  # Show \"log\" on jupyter notebook  \n",
    "]\n",
    "\n",
    "epoch = flags.epoch\n",
    "\n",
    "models = {\"main\": model}\n",
    "optimizers = {\"main\": optimizer}\n",
    "manager = IgniteExtensionsManager(\n",
    "    trainer,\n",
    "    models,\n",
    "    optimizers,\n",
    "    epoch,\n",
    "    extensions=extensions,\n",
    "    out_dir=str(out_dir),\n",
    "    # iters_per_epoch = len(train_loader)\n",
    "\n",
    "\n",
    ")\n",
    "# Save predictor.pt every epoch\n",
    "manager.extend(E.snapshot_object(predictor, \"predictor.pt\"),\n",
    "               trigger=(flags.snapshot_freq, \"iteration\"))\n",
    "# Check & Save best validation predictor.pt every epoch\n",
    "# manager.extend(E.snapshot_object(predictor, \"best_predictor.pt\"),\n",
    "#                trigger=MinValueTrigger(\"validation/main/nll\", trigger=(flags.snapshot_freq, \"iteration\")))\n",
    "# --- lr scheduler ---\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.7, patience=5, min_lr=1e-10)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer, gamma=0.99999)\n",
    "manager.extend(lambda manager: scheduler.step(), trigger=(1, \"iteration\"))\n",
    "# Show \"lr\" column in log\n",
    "manager.extend(E.observe_lr(optimizer=optimizer), trigger=log_trigger)\n",
    "\n",
    "# --- HACKING to fix ProgressBarNotebook bug ---\n",
    "manager.iteration = 0\n",
    "manager.iters_per_epoch = len(train_loader)\n",
    "\n",
    "trainer.run(train_loader, max_epochs=epoch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So what is happening in above training abstraction? Let's understand what each extension did.\n",
    "\n",
    "**Extensions** - Each role:\n",
    " - **`ProgressBar` (`ProgressBarNotebook`)**: Shows training progress in formatted style.\n",
    " - **`LogReport`**: Logging metrics reported by `ppe.reporter.report` (see `LyftMultiRegressor` for reporting point) method and save to **log** file. It automatically collects reported value in each iteration and saves the \"mean\" of reported value for regular frequency (for example every 1 epoch).\n",
    " - **`PrintReport` (`PrintReportNotebook`)**: Prints the value which `LogReport` collected in formatted style.\n",
    " - **`Evaluator`**: Evaluate on validation dataset.\n",
    " - **`snapshot_object`**: Saves the object. Here the `model` is saved in regular interval `flags.snapshot_freq`. Even you quit training using Ctrl+C without finishing all the epoch, the intermediate trained model is saved and you can use it for inference.\n",
    " - **lambda function with `scheduler.step()`**: You can invoke any function in regular interval specified by `trigger`. Here exponential decay of learning rate is applied by calling `scheduler.step()` every iteration.\n",
    " - **`observe_lr`**: `LogReport` will check optimizer's learning rate using this extension. So you can follow how the learning rate changed through the training.\n",
    "\n",
    "\n",
    "Such many functionalities can be \"added\" easily using extensions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtrain training history results really easily by just accessing `LogReport` class, which is useful for managing a lot of experiments during kaggle competitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = log_report.to_dataframe()\n",
    "df.to_csv(out_dir/\"log.csv\", index=False)\n",
    "df[[\"epoch\", \"iteration\", \"main/loss\", \"main/nll\", \"validation/main/loss\", \"validation/main/nll\", \"lr\", \"elapsed_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history log and model's weight are saved by \"extensions\" (`LogReport` and `E.snapshot_object` respectively) easily, which is a benefit of using training abstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see training results directory\n",
    "\n",
    "!ls results/multi_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Items to try\n",
    "\n",
    "This kernel shows demonstration run of the training (`debug=True`). You can try these things to see how the score changes at first\n",
    " - set debug=False to train with actual training dataset\n",
    " - change training hyperparameters (training epoch, change optimizer, scheduler learning rate etc...)\n",
    "   - Especially, just training much longer time may improve the score.\n",
    " \n",
    "To go further, these items may be nice to try:\n",
    " - Change the cnn model (now simple resnet18 is used as baseline modeling)\n",
    " - Training the model using full dataset: [lyft-full-training-set](https://www.kaggle.com/philculliton/lyft-full-training-set)\n",
    " - Write your own rasterizer to prepare input image as motivation explained in previous kernel.\n",
    " - Consider much better scheme to predict multi-trajectory\n",
    "    - The model just predicts multiple trajectory at the same time in this kernel, but it is possible to collapse \"trivial\" solution where all trajectory converges to same. How to avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next to go\n",
    "\n",
    "[Update 2020/9/6]<br/>\n",
    "Published prediction kernel: [Lyft: Prediction with multi-mode confidence](https://www.kaggle.com/corochann/lyft-prediction-with-multi-mode-confidence)<br/>\n",
    "Try yourself how good score you can get using only single model without ensemble! :)\n",
    "\n",
    "To understand the competition in more detail, please refer my other kernels too.\n",
    " - [Lyft: Comprehensive guide to start competition](https://www.kaggle.com/corochann/lyft-comprehensive-guide-to-start-competition)\n",
    " - [Lyft: Deep into the l5kit library](https://www.kaggle.com/corochann/lyft-deep-into-the-l5kit-library)\n",
    " - [Save your time, submit without kernel inference](https://www.kaggle.com/corochann/save-your-time-submit-without-kernel-inference)\n",
    " - [Lyft: pytorch implementation of evaluation metric](https://www.kaggle.com/corochann/lyft-pytorch-implementation-of-evaluation-metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reference\n",
    "\n",
    " - Paper of this Lyft Level 5 prediction dataset: [One Thousand and One Hours: Self-driving Motion Prediction Dataset](https://arxiv.org/abs/2006.14480)\n",
    " - [jpbremer/lyft-scene-visualisations](https://www.kaggle.com/jpbremer/lyft-scene-visualisations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\">If this kernel helps you, please upvote to keep me motivated :)<br>Thanks!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lyft: Deep into the l5kit library\n",
    "\n",
    "![](http://www.l5kit.org/_images/av.jpg)\n",
    "<cite>The image from L5Kit official document: <a href=\"http://www.l5kit.org/README.html\">http://www.l5kit.org/README.html</a></cite>\n",
    "\n",
    "Continued from the previous kernel [Lyft: Comprehensive guide to start competition](https://www.kaggle.com/corochann/lyft-comprehensive-guide-to-start-competition).\n",
    "\n",
    "In this kernel, I will look into the **raw data structures** and **l5kit library** in more detail **with code reading**. After understanding these, I hope you can arrange the data by yourself to build a better pipleline for motion prediction.\n",
    "\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "** [1. Understanding Rasterizer class](#rasterizer)** <br>\n",
    "** [2. Understanding EgoDataset/AgentDataset class](#ego_agent_dataset)** <br>\n",
    "** [3. Understanding raw data structures](#raw_data)** <br>\n",
    "\n",
    "\n",
    "The first part is same with previous kernel, please jump to [1. Understanding Rasterizer class](#rasterizer) for the main topic of this kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Competition description\n",
    "\n",
    " - Official page: [https://self-driving.lyft.com/level5/prediction/](https://self-driving.lyft.com/level5/prediction/)\n",
    "\n",
    "<blockquote>\n",
    "    The dataset consists of 170,000 scenes capturing the environment around the autonomous vehicle. Each scene encodes the state of the vehicleâ€™s surroundings at a given point in time.\n",
    "</blockquote>\n",
    "\n",
    "<div style=\"clear:both;display:table\">\n",
    "<img src=\"https://self-driving.lyft.com/wp-content/uploads/2020/06/motion_dataset_lrg_redux.gif\" style=\"width:45%;float:left\"/>\n",
    "<img src=\"https://self-driving.lyft.com/wp-content/uploads/2020/06/motion_dataset_2-1.png\" style=\"width:45%;float:left\"/>\n",
    "</div>\n",
    "\n",
    "<br/>\n",
    "<p><b>The goal of this competition is to predict other car/cyclist/pedestrian (called \"agent\")'s motion.</b><p>\n",
    "\n",
    "<img src=\"https://self-driving.lyft.com/wp-content/uploads/2020/06/diagram-prediction-1.jpg\" style=\"width:70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment setup\n",
    "\n",
    " - Please add \"[philculliton/kaggle-l5kit](https://www.kaggle.com/mathurinache/kaggle-l5kit)\" as utility script\n",
    " - Please add [lyft-config-files](https://www.kaggle.com/jpbremer/lyft-config-files) as dataset\n",
    " \n",
    "See previous kernel [Lyft: Comprehensive guide to start competition](https://www.kaggle.com/corochann/lyft-comprehensive-guide-to-start-competition) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:46:46.167086Z",
     "iopub.status.busy": "2022-09-11T02:46:46.166212Z",
     "iopub.status.idle": "2022-09-11T02:46:56.456323Z",
     "shell.execute_reply": "2022-09-11T02:46:56.455023Z",
     "shell.execute_reply.started": "2022-09-11T02:46:46.166973Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14604\\109800024.py:16: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:47:30.299416Z",
     "iopub.status.busy": "2022-09-11T02:47:30.298973Z",
     "iopub.status.idle": "2022-09-11T02:47:41.753629Z",
     "shell.execute_reply": "2022-09-11T02:47:41.752396Z",
     "shell.execute_reply.started": "2022-09-11T02:47:30.299375Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzarr\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01ml5kit\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01ml5kit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkedDataset, LocalDataManager\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01ml5kit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EgoDataset, AgentDataset\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01ml5kit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrasterization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_rasterizer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\l5kit\\data\\__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlabels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PERCEPTION_LABEL_TO_INDEX, PERCEPTION_LABELS, TL_FACE_LABEL_TO_INDEX, TL_FACE_LABELS\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal_data_manager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataManager, LocalDataManager\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmap_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MapAPI\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzarr_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AGENT_DTYPE, ChunkedDataset, FRAME_DTYPE, SCENE_DTYPE, TL_FACE_DTYPE\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzarr_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zarr_concat\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\l5kit\\data\\map_api.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01ml5kit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataManager\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_points\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroad_network_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoFrame, GlobalId, MapElement, MapFragment\n\u001b[0;32m     15\u001b[0m CACHE_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e5\u001b[39m)\n\u001b[0;32m     16\u001b[0m ENCODING \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\l5kit\\data\\proto\\road_network_pb2.py:36\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m     17\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     18\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroad_network.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml5kit.maps\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m   ,\n\u001b[0;32m     25\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[google_dot_protobuf_dot_descriptor__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,google_dot_protobuf_dot_empty__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n\u001b[0;32m     29\u001b[0m _ACCESSRESTRICTION_TYPE \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mEnumDescriptor(\n\u001b[0;32m     30\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml5kit.maps.AccessRestriction.Type\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     33\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[0;32m     34\u001b[0m   create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[0;32m     35\u001b[0m   values\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 36\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnumValueDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUNKNOWN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcreate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_create_key\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     41\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[0;32m     42\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO_RESTRICTION\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     43\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[0;32m     46\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[0;32m     47\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mONLY_HOV\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     48\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     50\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[0;32m     51\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[0;32m     52\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mONLY_BUS\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     53\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     55\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[0;32m     56\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[0;32m     57\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mONLY_BIKE\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     58\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     59\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     60\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[0;32m     61\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[0;32m     62\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mONLY_TURN\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     63\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     64\u001b[0m       \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     65\u001b[0m       create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key),\n\u001b[0;32m     66\u001b[0m   ],\n\u001b[0;32m     67\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     69\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m575\u001b[39m,\n\u001b[0;32m     70\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m672\u001b[39m,\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     72\u001b[0m _sym_db\u001b[38;5;241m.\u001b[39mRegisterEnumDescriptor(_ACCESSRESTRICTION_TYPE)\n\u001b[0;32m     74\u001b[0m _DAILYTIMEINTERVAL_DAYOFTHEWEEK \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mEnumDescriptor(\n\u001b[0;32m     75\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDayOfTheWeek\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     76\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml5kit.maps.DailyTimeInterval.DayOfTheWeek\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m976\u001b[39m,\n\u001b[0;32m    121\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\google\\protobuf\\descriptor.py:755\u001b[0m, in \u001b[0;36mEnumValueDescriptor.__new__\u001b[1;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, index, number,\n\u001b[0;32m    753\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m    754\u001b[0m             options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 755\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    756\u001b[0m   \u001b[38;5;66;03m# There is no way we can build a complete EnumValueDescriptor with the\u001b[39;00m\n\u001b[0;32m    757\u001b[0m   \u001b[38;5;66;03m# given parameters (the name of the Enum is not known, for example).\u001b[39;00m\n\u001b[0;32m    758\u001b[0m   \u001b[38;5;66;03m# Fortunately generated files just pass it to the EnumDescriptor()\u001b[39;00m\n\u001b[0;32m    759\u001b[0m   \u001b[38;5;66;03m# constructor, which will ignore it, so returning None is good enough.\u001b[39;00m\n\u001b[0;32m    760\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import zarr\n",
    "\n",
    "import l5kit\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "print(\"l5kit version:\", l5kit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:48:33.191996Z",
     "iopub.status.busy": "2022-09-11T02:48:33.191195Z",
     "iopub.status.idle": "2022-09-11T02:48:33.202266Z",
     "shell.execute_reply": "2022-09-11T02:48:33.201370Z",
     "shell.execute_reply.started": "2022-09-11T02:48:33.191951Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    "\n",
    "\n",
    "# Originally from https://www.kaggle.com/jpbremer/lyft-scene-visualisations by @jpbremer\n",
    "# Modified following:\n",
    "#  - Added to show timestamp\n",
    "#  - Do not show image, to only show animation.\n",
    "#  - Use blit=True.\n",
    "\n",
    "def animate_solution(images, timestamps=None):\n",
    "    def animate(i):\n",
    "        changed_artifacts = [im]\n",
    "        im.set_data(images[i])\n",
    "        if timestamps is not None:\n",
    "            time_text.set_text(timestamps[i])\n",
    "            changed_artifacts.append(im)\n",
    "        return tuple(changed_artifacts)\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(images[0])\n",
    "    if timestamps is not None:\n",
    "        time_text = ax.text(0.02, 0.95, \"\", transform=ax.transAxes)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(images), interval=60, blit=True)\n",
    "    \n",
    "    # To prevent plotting image inline.\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initial setup\n",
    "\n",
    "## Word definition\n",
    " - **\"Ego\"** is the host car which is recording/measuring the dataset.\n",
    " - **\"Agent\"** is the surronding car except \"Ego\" car.\n",
    " - **\"Frame\"** is the 1 image snapshot, where **\"Scene\"** is made of multiple frames of contious-time (video).\n",
    "\n",
    "## Class diagram\n",
    "<a id=\"class_diagram\"></a>\n",
    "<img src=\"https://storage.googleapis.com/kaggle-forum-message-attachments/987047/16744/l5kit_class.png\" width=\"600\" />\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading data\n",
    "\n",
    "Here we will only use the first dataset from the sample set. (sample.zarr data is used for visualization, please use train.zarr / validate.zarr / test.zarr for actual model training/validation/prediction.)<br/>\n",
    "We're building a `LocalDataManager` object. This will resolve relative paths from the config using the `L5KIT_DATA_FOLDER` env variable we have just set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:48:38.609738Z",
     "iopub.status.busy": "2022-09-11T02:48:38.608903Z",
     "iopub.status.idle": "2022-09-11T02:48:38.633422Z",
     "shell.execute_reply": "2022-09-11T02:48:38.631609Z",
     "shell.execute_reply.started": "2022-09-11T02:48:38.609674Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\"\n",
    "# get config\n",
    "cfg = load_config_data(\"/kaggle/input/lyft-config-files/visualisation_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:48:41.151936Z",
     "iopub.status.busy": "2022-09-11T02:48:41.151501Z",
     "iopub.status.idle": "2022-09-11T02:48:41.339476Z",
     "shell.execute_reply": "2022-09-11T02:48:41.337976Z",
     "shell.execute_reply.started": "2022-09-11T02:48:41.151893Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dm = LocalDataManager()\n",
    "dataset_path = dm.require('scenes/sample.zarr')\n",
    "zarr_dataset = ChunkedDataset(dataset_path)\n",
    "zarr_dataset.open()\n",
    "print(zarr_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"rasterizer\"></a>\n",
    "\n",
    "# 1. Understanding Rasterizer class\n",
    "\n",
    "## Rasterizer class\n",
    "\n",
    "The first topic I will introduce is \"Rasterizer\". This class supports 2 methods (See base [Rasterizer](https://github.com/lyft/l5kit/blob/master/l5kit/l5kit/rasterization/rasterizer.py) class).\n",
    "\n",
    " - `rasterize` method: to create (ch, height, width) format image. Basically this can be used for the input of prediciton model. It can have any number of channels.\n",
    " - `to_rgb` method: to convert image made by rasterize method into RGB image (ch=3, height, width).\n",
    " \n",
    "`l5kit` already provides several kinds of Rasterizer, each can be instantiated via [build_rasterizer](https://github.com/lyft/l5kit/blob/1e235b8617488e818be30cd7193d43588125bbab/l5kit/l5kit/rasterization/rasterizer_builder.py#L99) method with `cfg`. Let's see each class's role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:48:58.685488Z",
     "iopub.status.busy": "2022-09-11T02:48:58.684851Z",
     "iopub.status.idle": "2022-09-11T02:48:58.691609Z",
     "shell.execute_reply": "2022-09-11T02:48:58.690784Z",
     "shell.execute_reply.started": "2022-09-11T02:48:58.685447Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_rgb_image(dataset, index, title=\"\", ax=None):\n",
    "    \"\"\"Visualizes Rasterizer's RGB image\"\"\"\n",
    "    data = dataset[index]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(im[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:49:01.017458Z",
     "iopub.status.busy": "2022-09-11T02:49:01.016690Z",
     "iopub.status.idle": "2022-09-11T02:49:22.035263Z",
     "shell.execute_reply": "2022-09-11T02:49:22.034070Z",
     "shell.execute_reply.started": "2022-09-11T02:49:01.017411Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare all rasterizer and EgoDataset for each rasterizer\n",
    "rasterizer_dict = {}\n",
    "dataset_dict = {}\n",
    "\n",
    "rasterizer_type_list = [\"py_satellite\", \"satellite_debug\", \"py_semantic\", \"semantic_debug\", \"box_debug\", \"stub_debug\"]\n",
    "\n",
    "for i, key in enumerate(rasterizer_type_list):\n",
    "    # print(\"key\", key)\n",
    "    cfg[\"raster_params\"][\"map_type\"] = key\n",
    "    rasterizer_dict[key] = build_rasterizer(cfg, dm)\n",
    "    dataset_dict[key] = EgoDataset(cfg, zarr_dataset, rasterizer_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:49:28.602447Z",
     "iopub.status.busy": "2022-09-11T02:49:28.601958Z",
     "iopub.status.idle": "2022-09-11T02:49:29.980093Z",
     "shell.execute_reply": "2022-09-11T02:49:29.978896Z",
     "shell.execute_reply.started": "2022-09-11T02:49:28.602406Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "for i, key in enumerate([\"stub_debug\", \"satellite_debug\", \"semantic_debug\", \"box_debug\", \"py_satellite\", \"py_semantic\"]):\n",
    "    visualize_rgb_image(dataset_dict[key], index=0, title=f\"{key}: {type(rasterizer_dict[key]).__name__}\", ax=axes[i])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that\n",
    " - `StubRasterizer` is just for debugging, creates all black image with specified (height, width).\n",
    " - `BoxRasterizer` creates Ego (host car) as green box, and Agent as blue box.\n",
    " - `SatelliteRasterizer` draws satellite map.\n",
    " - `SemanticRasterizer` draws semantic map which contains lane & crosswalk information\n",
    " - `SatBoxRasterizer` = SatelliteRasterizer + BoxRasterizer\n",
    " - `SemBoxRasterizer` = SemanticRasterizer + BoxRasterizer\n",
    "\n",
    "\n",
    "Note that I guess Satellite image is NOT taken at the same time when host car moves, so the car on satellite image does NOT match with the car on drawn in BoxRasterizer!<br/>\n",
    "Satellite image is useful to get detailed information about the current place, but does not represent the current situation of car or traffic light etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Meaning of color in semantic map?\n",
    "\n",
    "Looking the code, I see that\n",
    "\n",
    " - default lane color is \"light yellow\" (255, 217, 82). [code](https://github.com/lyft/l5kit/blob/1e235b8617488e818be30cd7193d43588125bbab/l5kit/l5kit/rasterization/semantic_rasterizer.py#L198)\n",
    " - green, yellow, red color on lane is to show trafic light condition. [code](https://github.com/lyft/l5kit/blob/1e235b8617488e818be30cd7193d43588125bbab/l5kit/l5kit/rasterization/semantic_rasterizer.py#L199-L201)\n",
    " - orange box represents crosswalk. [code](https://github.com/lyft/l5kit/blob/1e235b8617488e818be30cd7193d43588125bbab/l5kit/l5kit/rasterization/semantic_rasterizer.py#L204-L211)\n",
    "\n",
    "Please refer below animation to verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-09-11T02:50:52.907598Z",
     "iopub.status.busy": "2022-09-11T02:50:52.907057Z",
     "iopub.status.idle": "2022-09-11T02:50:52.917269Z",
     "shell.execute_reply": "2022-09-11T02:50:52.915826Z",
     "shell.execute_reply.started": "2022-09-11T02:50:52.907546Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_animate_for_indexes(dataset, indexes):\n",
    "    images = []\n",
    "    timestamps = []\n",
    "\n",
    "    for idx in indexes:\n",
    "        data = dataset[idx]\n",
    "        im = data[\"image\"].transpose(1, 2, 0)\n",
    "        im = dataset.rasterizer.to_rgb(im)\n",
    "        target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "        center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "        draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n",
    "        clear_output(wait=True)\n",
    "        images.append(PIL.Image.fromarray(im[::-1]))\n",
    "        timestamps.append(data[\"timestamp\"])\n",
    "\n",
    "    anim = animate_solution(images, timestamps)\n",
    "    return anim\n",
    "\n",
    "def create_animate_for_scene(dataset, scene_idx):\n",
    "    indexes = dataset.get_scene_indices(scene_idx)\n",
    "    return create_animate_for_indexes(dataset, indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Car was stopping during traffic light is red, and starts once traffic becomes green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-11T02:50:54.896428Z",
     "iopub.status.busy": "2022-09-11T02:50:54.895941Z",
     "iopub.status.idle": "2022-09-11T02:50:55.337033Z",
     "shell.execute_reply": "2022-09-11T02:50:55.335534Z",
     "shell.execute_reply.started": "2022-09-11T02:50:54.896375Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset_dict[\"py_semantic\"]\n",
    "scene_idx = 34\n",
    "anim = create_animate_for_scene(dataset, scene_idx)\n",
    "print(\"scene_idx\", scene_idx)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scene_idx = 0\n",
    "print(\"scene_idx\", scene_idx)\n",
    "anim = create_animate_for_scene(dataset, scene_idx)\n",
    "display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see car comes to the red traffic light and stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scene_idx = 1\n",
    "print(\"scene_idx\", scene_idx)\n",
    "anim = create_animate_for_scene(dataset, scene_idx)\n",
    "display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Traffic light is always red in the scene. But agent car which turns right moves, which is allowed in US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scene_idx = 2\n",
    "print(\"scene_idx\", scene_idx)\n",
    "anim = create_animate_for_scene(dataset, scene_idx)\n",
    "display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Observation\n",
    "\n",
    "We understand that `SemanticRasterizer` creates each lane, traffic light status or car's box as **\"RGB\" image**.<br/>\n",
    "However it is not necessary for CNN model to input RGB image. Any information representation for each channel is allowed.\n",
    "\n",
    "I guess different representation may boost the prediction model's performance, which you need to write your own rasterizer.<br/>\n",
    "For example represent own car, other car, lane and traffic light in different channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"ego_agent_dataset\"></a>\n",
    "# 2. Understanding EgoDataset/AgentDataset class\n",
    "\n",
    "Instead of working with raw data, L5Kit provides PyTorch ready datasets.\n",
    "It's much easier to use this wrapped dataset class to access data.\n",
    "\n",
    "2 dataset class is implemented.\n",
    "\n",
    " - **EgoDataset**: this dataset iterates over the AV (Autonomous Vehicle) annotations\n",
    " - **AgentDataset**: this dataset iterates over other agents annotations\n",
    "\n",
    "Let's see each class in detail. What kind of attributes/methods they have? What kind of data structure for each attributes?\n",
    "\n",
    "\n",
    "As written in [Class diagram](#class_diagram), both classes are instantiated by:\n",
    " - `cfg`: configuration file\n",
    " - `ChunkedDataset`: Internal data class which holds 4 raw data `scenes`, `frames`, `agents` and `tl_faces` (described later).\n",
    " - `rasterizer`: Rasterizer converts raw data into image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## EgoDataset\n",
    "\n",
    "\n",
    "The implementation code is in [ego.py](https://github.com/lyft/l5kit/blob/master/l5kit/l5kit/dataset/ego.py).<br/>\n",
    "\n",
    "### Internal data structure\n",
    "<img src=\"https://storage.googleapis.com/kaggle-forum-message-attachments/990934/16784/l5kit_ego_dataset.png\" width=\"600\" />\n",
    "\n",
    "`EgoDataset` consists of multiple scenes. Each scene is usually 25 sec consecutive events, consists of multiple frames.<br/>\n",
    "A frame reprsents specific time's snapshot. Snapshot is taken in 0.1 sec interval, so usually 1 scene is made of about 250 frames.\n",
    "\n",
    "Blue box represents each scene, orange box represents each frame as well as each data index.\n",
    "\n",
    "When we access dataset by index i, i-th frame is returned. Frames are concatenated by multiple scenes, we different i-th index points different scene.\n",
    "The point where the scene will change is represented by `cumulative_sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "semantic_rasterizer = rasterizer_dict[\"py_semantic\"]\n",
    "dataset = dataset_dict[\"py_semantic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# It shows the split point of each scene.\n",
    "print(\"cumulative_sizes\", dataset.cumulative_sizes)\n",
    "\n",
    "# How's the length of each scene?\n",
    "print(\"Each scene's length\", dataset.cumulative_sizes[1:] - dataset.cumulative_sizes[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems 1 scene usually consists of 248, 249 frames.\n",
    "\n",
    "Now Let's check each method:\n",
    "\n",
    "### getitem, get_frame\n",
    "\n",
    "When we access data by index as `dataset[i]`, `__getitem__` is called and l5kit internally calls `get_frame`.<br/>\n",
    "This method preprocesses the data and returns many features as dict format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "\n",
    "print(\"dataset[0]=data is \", type(data))\n",
    "\n",
    "def _describe(value):\n",
    "    if hasattr(value, \"shape\"):\n",
    "        return f\"{type(value).__name__:20} shape={value.shape}\"\n",
    "    else:\n",
    "        return f\"{type(value).__name__:20} value={value}\"\n",
    "\n",
    "for key, value in data.items():\n",
    "    print(\"  \", f\"{key:25}\", _describe(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each attribute represents follows (The data structure is same for `AgentDataset`, and I included explanation for `AgentDataset` as well):\n",
    "\n",
    " - image: image drawn by Rasterizer. As you saw on the top of this kernel. This is usually be the **input image for CNN**\n",
    " - target_positions: The \"Ego car\" or \"Agent (car/cyclist/pedestrian etc)\"'s future position. This is **the value to predict in this competition (not for Ego car's, but for Agents)**.\n",
    " - target_yaws: The Ego car's future yaw, to represent heading direction.\n",
    " - target_availabilities: flag to represent this is valid or not. Only flag=1 is used for competition evaluation.\n",
    " - history_positions: Past positions\n",
    " - history_yaws: Past yaws\n",
    " - history_availabilities:\n",
    " - world_to_image: 3x3 transformation matrix to convert world-coordinate into pixel-coordinate.\n",
    " - track_id: Unique ID for each Agent. `None` for Ego car.\n",
    " - timestamp: timestamp for current frame.\n",
    " - centroid: current center position\n",
    " - yaw: current direction\n",
    " - extent: Ego car or Agent's size. The car is not represented as point, but should be cared as dot box to include size information on the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you don't know what is \"yaw\", please refer [wikipedia](https://en.wikipedia.org/wiki/Yaw_(rotation)).\n",
    "\n",
    "<div style=\"clear:both;display:table\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/54/Flight_dynamics_with_text.png\" style=\"width:30%;float:left\"/>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/96/Aileron_yaw.gif\" style=\"width:30%;float:left\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next method to check is **`get_scene_indices` and `get_scene_dataset`**.\n",
    "\n",
    "- `get_scene_indices(i)` method will return i-th scene's frame indices.\n",
    "- `get_scene_dataset(i)` method will return other `EgoDataset` which only contains i-th scene's frames.\n",
    "   - As you can see below, it contains only 1 scene when we visualize whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scene_index = 0\n",
    "frame_indices = dataset.get_scene_indices(scene_index)\n",
    "print(f\"frame_indices for scene {scene_index} = {frame_indices}\")\n",
    "\n",
    "scene_dataset = dataset.get_scene_dataset(scene_index)\n",
    "print(f\"scene_dataset {type(scene_dataset).__name__}, length {len(scene_dataset)}\")\n",
    "\n",
    "# Animate whole \"scene_dataset\"\n",
    "create_animate_for_indexes(scene_dataset, np.arange(len(scene_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Last method I will explain is `get_frame_indices`.\n",
    "\n",
    "`get_frame_indices(j)` will return all the `dataset[i]` whose frame points to `j`-th frame.<br/>\n",
    "For `EgoDataset`, these are same and i-th data always points to i-th frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "frame_idx = 10\n",
    "indices = dataset.get_frame_indices(frame_idx)\n",
    "\n",
    "# These are same for EgoDataset!\n",
    "print(f\"frame_idx = {frame_idx}, indices = {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## AgentDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "The implementation code is in [agent.py](https://github.com/lyft/l5kit/blob/master/l5kit/l5kit/dataset/agent.py).<br/>\n",
    "\n",
    "### Internal data structure\n",
    "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F518134%2F9ea7ae3af0037edb95cc0effe361568f%2Fl5kit_agent_dataset.png?generation=1598749570093014&alt=media\" width=\"600\" />\n",
    "\n",
    "`AgentDataset` consists of same multiple scenes as `EgoDataset`.\n",
    "Always 1 host Ego car exists in each frame, however the number of agent differ.<br/>\n",
    "It can contain multiple agents in 1 frame, or even 0 agents in some of the frames.\n",
    "\n",
    "See above figure, AgentDataset contains all the agents' frame in different index.<br/>\n",
    "If you access `dataset[i]`, it returns unique scene's unique frame's unique agent's information.\n",
    "\n",
    "In the figure, the x-axis represents same frame = same timestamp, and the y-axis represents agents.\n",
    "Blue box represents each scene, black box represents same frame, and orange box represents each data index.\n",
    "\n",
    "The point where the scene will change is represented by `cumulative_sizes`, same as `EgoDataset`.\n",
    "The point where the **agent** will change is represented by **`cumulative_sizes_agents`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "semantic_rasterizer = rasterizer_dict[\"py_semantic\"]\n",
    "agent_dataset = AgentDataset(cfg, zarr_dataset, semantic_rasterizer)\n",
    "\n",
    "print(f\"EgoDataset size {len(dataset)}, AgentDataset size {len(agent_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`AgentDataset` size is usually much bigger than `EgoDataset`, since multiple agents exist in 1 frame.\n",
    "\n",
    "The returned data structure by `__getitem__` is same with `EgoDataset`. Please refer `EgoDataset` exlanation for details of each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The returned data structure is same.\n",
    "data = agent_dataset[0]\n",
    "\n",
    "print(\"agent_dataset[0]=data is \", type(data))\n",
    "\n",
    "def _describe(value):\n",
    "    if hasattr(value, \"shape\"):\n",
    "        return f\"{type(value).__name__:20} shape={value.shape}\"\n",
    "    else:\n",
    "        return f\"{type(value).__name__:20} value={value}\"\n",
    "\n",
    "for key, value in data.items():\n",
    "    print(\"  \", f\"{key:25}\", _describe(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Same methods with `EgoDataset` are supported:\n",
    "\n",
    "- `get_scene_indices(i)` method will return i-th scene's frame indices. However it contains multiple agents and **thus contains same frame multiple times**.\n",
    "- `get_scene_dataset(i)` method will return other `AgentDataset` which only contains i-th scene's frames.\n",
    "\n",
    "Please see below animation.<br/>\n",
    "Same scene contains multiple agents, thus contains multiple same frames. Recommended to press \"Next frame\" button manually to how timestamp=frame evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scene_index = 3\n",
    "frame_indices = agent_dataset.get_scene_indices(scene_index)\n",
    "print(f\"frame_indices for scene {scene_index} = {frame_indices}\")\n",
    "\n",
    "scene_dataset = agent_dataset.get_scene_dataset(scene_index)\n",
    "print(f\"scene_dataset {type(scene_dataset).__name__}, length {len(scene_dataset)}\")\n",
    "\n",
    "# Animate whole \"scene_dataset\"\n",
    "create_animate_for_indexes(scene_dataset, np.arange(len(scene_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`get_frame_indices` returns the index which contains same frame.\n",
    "\n",
    "`EgoDataset` return was trivial, but `AgentDataset` may return multiple indices since several agents exist in each frame. \n",
    "Let's see example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print(i, agent_dataset.get_frame_indices(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "frame_indices = agent_dataset.get_frame_indices(648)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(frame_indices), figsize=(15, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(len(frame_indices)):\n",
    "    index = frame_indices[i]\n",
    "    t = agent_dataset[index][\"timestamp\"]\n",
    "    # Timestamp is same for same frame.\n",
    "    print(f\"timestamp = {t}\")\n",
    "    visualize_rgb_image(agent_dataset, index=index, title=f\"index={index}\", ax=axes[i])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a id=\"raw_data\"></a>\n",
    "# 3. Understanding raw data structures\n",
    "\n",
    "We have seen `EgoDataset` & `AgentDataset` functionality, however it is a wrapped methods and we have still not understanding how the raw data is made.<br/>\n",
    "Let's focus on `ChunkDataset` (`zarr_dataset` variable) here, it contains **scenes, frames, agents, tl_faces** attributes, which are the raw structured array data.\n",
    "\n",
    "Each data structure definition can be checked at [here](https://github.com/lyft/l5kit/blob/master/data_format.md#2020-lyft-competition-dataset-format).\n",
    "\n",
    "### Overfiew figure\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-forum-message-attachments/991027/16786/l5kit_chunked_dataset.png\" width=\"600\" />\n",
    "\n",
    "### scenes\n",
    "\n",
    "```\n",
    "SCENE_DTYPE = [\n",
    "    (\"frame_index_interval\", np.int64, (2,)),\n",
    "    (\"host\", \"<U16\"),  # Unicode string up to 16 chars\n",
    "    (\"start_time\", np.int64),\n",
    "    (\"end_time\", np.int64),\n",
    "]\n",
    "```\n",
    "\n",
    "`scenes` data contains each scene's information.<br/>\n",
    "Each scene basically owns a reference to the frames and consists of following information:\n",
    "\n",
    " - frame_index_interval: frame index for this scene.\n",
    " - host: unique name of host car.\n",
    " - start_time, end_time: timestamp for start & end of scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"scenes\", zarr_dataset.scenes)\n",
    "print(\"scenes[0]\", zarr_dataset.scenes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we understand that scene 0 consists of frame 0~248. Let's see go to see frame.\n",
    "\n",
    "### frames\n",
    "\n",
    "```\n",
    "FRAME_DTYPE = [\n",
    "    (\"timestamp\", np.int64),\n",
    "    (\"agent_index_interval\", np.int64, (2,)),\n",
    "    (\"traffic_light_faces_index_interval\", np.int64, (2,)),\n",
    "    (\"ego_translation\", np.float64, (3,)),\n",
    "    (\"ego_rotation\", np.float64, (3, 3)),\n",
    "]\n",
    "```\n",
    "\n",
    "`frames` data contains each frame's information.<br/>\n",
    "Each frame owns a reference to the agents & traffic_light_faces.<br/>\n",
    "It consists of following information:\n",
    "\n",
    " - timestamp: timestamp of this frame.\n",
    " - agent_index_interval: agent index for this frame.\n",
    " - traffic_light_faces_index_interval: traffic light faces index for this frame.\n",
    " - ego_translation, ego_rotation: position & direction of the host Ego car.\n",
    "\n",
    "frame does not contain any \"image\" information. image is created by Rasterizer by just the world coordinate position information known by `ego_translation` using `MapAPI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"frames\", zarr_dataset.frames)\n",
    "print(\"frames[0]\", zarr_dataset.frames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that frame 0 contains\n",
    " - agents for [0, 38).\n",
    " - traffic lights for [0, 0) (No traffic light included in this frame)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### agents\n",
    "\n",
    "```\n",
    "AGENT_DTYPE = [\n",
    "    (\"centroid\", np.float64, (2,)),\n",
    "    (\"extent\", np.float32, (3,)),\n",
    "    (\"yaw\", np.float32),\n",
    "    (\"velocity\", np.float32, (2,)),\n",
    "    (\"track_id\", np.uint64),\n",
    "    (\"label_probabilities\", np.float32, (len(LABELS),)),\n",
    "]\n",
    "```\n",
    "\n",
    "`agents` data contains **each frame's agent (vehicles, cyclists and pedestrians)** information.<br/>\n",
    "It consists of following information:\n",
    "\n",
    " - centroid: position of the agent.\n",
    " - extent: the size of the agent.\n",
    " - yaw: direction of the agent.\n",
    " - velocity: current velocity of the agent.\n",
    " - track_id: unique id to represent same agent within **different frames**.\n",
    " - label_probabilities: The agent is identification is automated by using already trained percenption network. Thus what kind of type is provided by predicted proability.\n",
    "\n",
    "The label definition can be found [here](https://github.com/lyft/l5kit/blob/1e235b8617488e818be30cd7193d43588125bbab/l5kit/l5kit/data/labels.py#L1-L19).\n",
    "We can understand the agent is either a car, a cyclist, a pedestrian etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"agents\", zarr_dataset.agents)\n",
    "print(\"agents[0]\", zarr_dataset.agents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### traffic_light_faces\n",
    "\n",
    "```\n",
    "TL_FACE_DTYPE = [\n",
    "    (\"face_id\", \"<U16\"),\n",
    "    (\"traffic_light_id\", \"<U16\"),\n",
    "    (\"traffic_light_face_status\", np.float32, (len(TL_FACE_LABELS,))),\n",
    "]\n",
    "```\n",
    "\n",
    "`tl_faces` data contains **each frame's traffic light** information.<br/>\n",
    "It consists of following information:\n",
    "\n",
    " - face_id: unique id for the traffic light bulb. Note that unlike agent, this traffic light is unique on the map for all scenes.\n",
    " - traffic_light_id: represent a traffic light status, e.g. the light is green/yellow/red etc. See [protocol buffer definition](https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/data/proto/road_network.proto#L615) for details.\n",
    " - traffic_light_status: 3-dim array. I guess each represents the condition of green/yellow/red light. Condition definition is [here](https://github.com/lyft/l5kit/blob/1e235b8617488e818be30cd7193d43588125bbab/l5kit/l5kit/data/labels.py#L23-L27), i.e., Active/Inactive/Unknown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"tl_faces\", zarr_dataset.tl_faces)\n",
    "print(\"tl_faces[0]\", zarr_dataset.tl_faces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Next to go\n",
    "\n",
    "That's all for going deep into the l5kit library.\n",
    "\n",
    "We can write your own rasterizer or even manually handling raw data to create more informative input/output data for prediction model to achieve high accuracy.\n",
    "\n",
    "**[Update] I wrote a kernel to train prediction models for competition submission as next topic to try!**\n",
    " - **[Lyft: Training with multi-mode confidence](https://www.kaggle.com/corochann/lyft-training-with-multi-mode-confidence)**\n",
    " - [Lyft: Prediction with multi-mode confidence](https://www.kaggle.com/corochann/lyft-prediction-with-multi-mode-confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Further reference\n",
    "\n",
    " - Paper of this Lyft Level 5 prediction dataset: [One Thousand and One Hours: Self-driving Motion Prediction Dataset](https://arxiv.org/abs/2006.14480)\n",
    " - [jpbremer/lyft-scene-visualisations](https://www.kaggle.com/jpbremer/lyft-scene-visualisations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:red\">If this kernel helps you, please upvote to keep me motivated :)<br>Thanks!</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
